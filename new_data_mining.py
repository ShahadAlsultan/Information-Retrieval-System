# -*- coding: utf-8 -*-
"""new data mining

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q3W_CRfUXilBrVMjfSx9HVLdxIZqbqf5
"""

#Shahad
!pip install nltk
import nltk

# Download the required resource
nltk.download('averaged_perceptron_tagger_eng')

import nltk
from nltk.corpus import stopwords, wordnet
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer, PorterStemmer

# ✅ تحميل الموارد المطلوبة من NLTK
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

# Download 'punkt_tab' for sentence tokenization:
nltk.download('punkt_tab') # This line is added to download the required data package.

# ✅ تهيئة أدوات Lemmatization و Stemming
lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()

# ✅ تحميل قائمة stop words
stop_words = set(stopwords.words('english'))

# ✅ تحديد مسار ملف Keeta.txt
file_path = "/content/Ninjanew.txt"

# ✅ قراءة الملف النصي
with open(file_path, "r", encoding="utf-8") as file:
    text = file.read().lower()  # تحويل النص إلى lowercase

# ✅ تقسيم النص إلى كلمات (`Tokenization`)
words = word_tokenize(text)

# ✅ إزالة stop words
filtered_words = [word for word in words if word not in stop_words]

# ✅ تحويل وسوم الكلمات لـ WordNet حتى يكون Lemmatization أدق
def get_wordnet_pos(word):
    """تحويل وسوم POS الخاصة بـ NLTK إلى تنسيق WordNet"""
    from nltk.tag import pos_tag
    tag = pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ, "N": wordnet.NOUN, "V": wordnet.VERB, "R": wordnet.ADV}
    return tag_dict.get(tag, wordnet.NOUN)  # الافتراضي: NOUN

# ✅ تطبيق Lemmatization
lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in filtered_words]

# ✅ تطبيق Stemming بعد Lemmatization
stemmed_words = [stemmer.stem(word) for word in lemmatized_words]

# ✅ إعادة تجميع النص بعد Lemmatization و Stemming
lemmatized_text = " ".join(lemmatized_words)
stemmed_text = " ".join(stemmed_words)

# ✅ طباعة النص بعد Lemmatization و Stemming
print("\n✅ Text after Lemmatization \n", lemmatized_text)
print("\n✅ Text after Stemming \n", stemmed_text)

#Shahad
!pip install nltk
import nltk

# Download the required resource
nltk.download('averaged_perceptron_tagger_eng')

import nltk
from nltk.corpus import stopwords, wordnet
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer, PorterStemmer

# ✅ تحميل الموارد المطلوبة من NLTK
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

# Download 'punkt_tab' for sentence tokenization:
nltk.download('punkt_tab') # This line is added to download the required data package.

# ✅ تهيئة أدوات Lemmatization و Stemming
lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()

# ✅ تحميل قائمة stop words
stop_words = set(stopwords.words('english'))

# ✅ تحديد مسار ملف Keeta.txt
file_path = "/content/Nananew.txt"

# ✅ قراءة الملف النصي
with open(file_path, "r", encoding="utf-8") as file:
    text = file.read().lower()  # تحويل النص إلى lowercase

# ✅ تقسيم النص إلى كلمات (`Tokenization`)
words = word_tokenize(text)

# ✅ إزالة stop words
filtered_words = [word for word in words if word not in stop_words]

# ✅ تحويل وسوم الكلمات لـ WordNet حتى يكون Lemmatization أدق
def get_wordnet_pos(word):
    """تحويل وسوم POS الخاصة بـ NLTK إلى تنسيق WordNet"""
    from nltk.tag import pos_tag
    tag = pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ, "N": wordnet.NOUN, "V": wordnet.VERB, "R": wordnet.ADV}
    return tag_dict.get(tag, wordnet.NOUN)  # الافتراضي: NOUN

# ✅ تطبيق Lemmatization
lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in filtered_words]

# ✅ تطبيق Stemming بعد Lemmatization
stemmed_words = [stemmer.stem(word) for word in lemmatized_words]

# ✅ إعادة تجميع النص بعد Lemmatization و Stemming
lemmatized_text = " ".join(lemmatized_words)
stemmed_text = " ".join(stemmed_words)

# ✅ طباعة النص بعد Lemmatization و Stemming
print("\n✅ Text after Lemmatization \n", lemmatized_text)
print("\n✅ Text after Stemming \n", stemmed_text)

#Shahad
!pip install nltk
import nltk

# Download the required resource
nltk.download('averaged_perceptron_tagger_eng')

import nltk
from nltk.corpus import stopwords, wordnet
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer, PorterStemmer

# ✅ تحميل الموارد المطلوبة من NLTK
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

# Download 'punkt_tab' for sentence tokenization:
nltk.download('punkt_tab') # This line is added to download the required data package.

# ✅ تهيئة أدوات Lemmatization و Stemming
lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()

# ✅ تحميل قائمة stop words
stop_words = set(stopwords.words('english'))

# ✅ تحديد مسار ملف Keeta.txt
file_path = "/content/Marsoolnew.txt"

# ✅ قراءة الملف النصي
with open(file_path, "r", encoding="utf-8") as file:
    text = file.read().lower()  # تحويل النص إلى lowercase

# ✅ تقسيم النص إلى كلمات (`Tokenization`)
words = word_tokenize(text)

# ✅ إزالة stop words
filtered_words = [word for word in words if word not in stop_words]

# ✅ تحويل وسوم الكلمات لـ WordNet حتى يكون Lemmatization أدق
def get_wordnet_pos(word):
    """تحويل وسوم POS الخاصة بـ NLTK إلى تنسيق WordNet"""
    from nltk.tag import pos_tag
    tag = pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ, "N": wordnet.NOUN, "V": wordnet.VERB, "R": wordnet.ADV}
    return tag_dict.get(tag, wordnet.NOUN)  # الافتراضي: NOUN

# ✅ تطبيق Lemmatization
lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in filtered_words]

# ✅ تطبيق Stemming بعد Lemmatization
stemmed_words = [stemmer.stem(word) for word in lemmatized_words]

# ✅ إعادة تجميع النص بعد Lemmatization و Stemming
lemmatized_text = " ".join(lemmatized_words)
stemmed_text = " ".join(stemmed_words)

# ✅ طباعة النص بعد Lemmatization و Stemming
print("\n✅ Text after Lemmatization \n", lemmatized_text)
print("\n✅ Text after Stemming \n", stemmed_text)

#Shahad
!pip install nltk
import nltk

# Download the required resource
nltk.download('averaged_perceptron_tagger_eng')

import nltk
from nltk.corpus import stopwords, wordnet
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer, PorterStemmer

# ✅ تحميل الموارد المطلوبة من NLTK
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

# Download 'punkt_tab' for sentence tokenization:
nltk.download('punkt_tab') # This line is added to download the required data package.

# ✅ تهيئة أدوات Lemmatization و Stemming
lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()

# ✅ تحميل قائمة stop words
stop_words = set(stopwords.words('english'))

# ✅ تحديد مسار ملف Keeta.txt
file_path = "/content/keetanew.txt"

# ✅ قراءة الملف النصي
with open(file_path, "r", encoding="utf-8") as file:
    text = file.read().lower()  # تحويل النص إلى lowercase

# ✅ تقسيم النص إلى كلمات (`Tokenization`)
words = word_tokenize(text)

# ✅ إزالة stop words
filtered_words = [word for word in words if word not in stop_words]

# ✅ تحويل وسوم الكلمات لـ WordNet حتى يكون Lemmatization أدق
def get_wordnet_pos(word):
    """تحويل وسوم POS الخاصة بـ NLTK إلى تنسيق WordNet"""
    from nltk.tag import pos_tag
    tag = pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ, "N": wordnet.NOUN, "V": wordnet.VERB, "R": wordnet.ADV}
    return tag_dict.get(tag, wordnet.NOUN)  # الافتراضي: NOUN

# ✅ تطبيق Lemmatization
lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in filtered_words]

# ✅ تطبيق Stemming بعد Lemmatization
stemmed_words = [stemmer.stem(word) for word in lemmatized_words]

# ✅ إعادة تجميع النص بعد Lemmatization و Stemming
lemmatized_text = " ".join(lemmatized_words)
stemmed_text = " ".join(stemmed_words)

# ✅ طباعة النص بعد Lemmatization و Stemming
print("\n✅ Text after Lemmatization \n", lemmatized_text)
print("\n✅ Text after Stemming \n", stemmed_text)

#Shahad
!pip install nltk
import nltk

# Download the required resource
nltk.download('averaged_perceptron_tagger_eng')

import nltk
from nltk.corpus import stopwords, wordnet
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer, PorterStemmer

# ✅ تحميل الموارد المطلوبة من NLTK
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

# Download 'punkt_tab' for sentence tokenization:
nltk.download('punkt_tab') # This line is added to download the required data package.

# ✅ تهيئة أدوات Lemmatization و Stemming
lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()

# ✅ تحميل قائمة stop words
stop_words = set(stopwords.words('english'))

# ✅ تحديد مسار ملف Keeta.txt
file_path = "/content/Hungerstationnew.txt"

# ✅ قراءة الملف النصي
with open(file_path, "r", encoding="utf-8") as file:
    text = file.read().lower()  # تحويل النص إلى lowercase

# ✅ تقسيم النص إلى كلمات (`Tokenization`)
words = word_tokenize(text)

# ✅ إزالة stop words
filtered_words = [word for word in words if word not in stop_words]

# ✅ تحويل وسوم الكلمات لـ WordNet حتى يكون Lemmatization أدق
def get_wordnet_pos(word):
    """تحويل وسوم POS الخاصة بـ NLTK إلى تنسيق WordNet"""
    from nltk.tag import pos_tag
    tag = pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ, "N": wordnet.NOUN, "V": wordnet.VERB, "R": wordnet.ADV}
    return tag_dict.get(tag, wordnet.NOUN)  # الافتراضي: NOUN

# ✅ تطبيق Lemmatization
lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in filtered_words]

# ✅ تطبيق Stemming بعد Lemmatization
stemmed_words = [stemmer.stem(word) for word in lemmatized_words]

# ✅ إعادة تجميع النص بعد Lemmatization و Stemming
lemmatized_text = " ".join(lemmatized_words)
stemmed_text = " ".join(stemmed_words)

# ✅ طباعة النص بعد Lemmatization و Stemming
print("\n✅ Text after Lemmatization \n", lemmatized_text)
print("\n✅ Text after Stemming \n", stemmed_text)

#Shahad
!pip install nltk
import nltk

# Download the required resource
nltk.download('averaged_perceptron_tagger_eng')

import nltk
from nltk.corpus import stopwords, wordnet
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer, PorterStemmer

# ✅ تحميل الموارد المطلوبة من NLTK
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

# Download 'punkt_tab' for sentence tokenization:
nltk.download('punkt_tab') # This line is added to download the required data package.

# ✅ تهيئة أدوات Lemmatization و Stemming
lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()

# ✅ تحميل قائمة stop words
stop_words = set(stopwords.words('english'))

# ✅ تحديد مسار ملف Keeta.txt
file_path = "/content/Jaheznew.txt"

# ✅ قراءة الملف النصي
with open(file_path, "r", encoding="utf-8") as file:
    text = file.read().lower()  # تحويل النص إلى lowercase

# ✅ تقسيم النص إلى كلمات (`Tokenization`)
words = word_tokenize(text)

# ✅ إزالة stop words
filtered_words = [word for word in words if word not in stop_words]

# ✅ تحويل وسوم الكلمات لـ WordNet حتى يكون Lemmatization أدق
def get_wordnet_pos(word):
    """تحويل وسوم POS الخاصة بـ NLTK إلى تنسيق WordNet"""
    from nltk.tag import pos_tag
    tag = pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ, "N": wordnet.NOUN, "V": wordnet.VERB, "R": wordnet.ADV}
    return tag_dict.get(tag, wordnet.NOUN)  # الافتراضي: NOUN

# ✅ تطبيق Lemmatization
lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in filtered_words]

# ✅ تطبيق Stemming بعد Lemmatization
stemmed_words = [stemmer.stem(word) for word in lemmatized_words]

# ✅ إعادة تجميع النص بعد Lemmatization و Stemming
lemmatized_text = " ".join(lemmatized_words)
stemmed_text = " ".join(stemmed_words)

# ✅ طباعة النص بعد Lemmatization و Stemming
print("\n✅ Text after Lemmatization \n", lemmatized_text)
print("\n✅ Text after Stemming \n", stemmed_text)

"""# New Section"""

from google.colab import drive
drive.mount('/content/drive')

#Fatimah
!pip install nltk
import nltk

# Download the required resource
nltk.download('averaged_perceptron_tagger_eng')

import nltk
from nltk.corpus import stopwords, wordnet
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer, PorterStemmer

# ✅ تحميل الموارد المطلوبة من NLTK
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

# Download 'punkt_tab' for sentence tokenization:
nltk.download('punkt_tab') # This line is added to download the required data package.

# ✅ تهيئة أدوات Lemmatization و Stemming
lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()

# ✅ تحميل قائمة stop words
stop_words = set(stopwords.words('english'))

# ✅ تحديد مسار ملف Keeta.txt
file_path = "/content/Adidas.txt"

# ✅ قراءة الملف النصي
with open(file_path, "r", encoding="utf-8") as file:
    text = file.read().lower()  # تحويل النص إلى lowercase

# ✅ تقسيم النص إلى كلمات (`Tokenization`)
words = word_tokenize(text)

# ✅ إزالة stop words
filtered_words = [word for word in words if word not in stop_words]

# ✅ تحويل وسوم الكلمات لـ WordNet حتى يكون Lemmatization أدق
def get_wordnet_pos(word):
    """تحويل وسوم POS الخاصة بـ NLTK إلى تنسيق WordNet"""
    from nltk.tag import pos_tag
    tag = pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ, "N": wordnet.NOUN, "V": wordnet.VERB, "R": wordnet.ADV}
    return tag_dict.get(tag, wordnet.NOUN)  # الافتراضي: NOUN

# ✅ تطبيق Lemmatization
lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in filtered_words]

# ✅ تطبيق Stemming بعد Lemmatization
stemmed_words = [stemmer.stem(word) for word in lemmatized_words]

# ✅ إعادة تجميع النص بعد Lemmatization و Stemming
lemmatized_text = " ".join(lemmatized_words)
stemmed_text = " ".join(stemmed_words)

# ✅ طباعة النص بعد Lemmatization و Stemming
print("\n✅ Text after Lemmatization \n", lemmatized_text)
print("\n✅ Text after Stemming \n", stemmed_text)

#Zainb
!pip install nltk
import nltk

# Download the required resource
nltk.download('averaged_perceptron_tagger_eng')

import nltk
from nltk.corpus import stopwords, wordnet
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer, PorterStemmer

# ✅ تحميل الموارد المطلوبة من NLTK
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

# Download 'punkt_tab' for sentence tokenization:
nltk.download('punkt_tab') # This line is added to download the required data package.

# ✅ تهيئة أدوات Lemmatization و Stemming
lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()

# ✅ تحميل قائمة stop words
stop_words = set(stopwords.words('english'))

# ✅ تحديد مسار ملف Keeta.txt
file_path = "/content/Polo_Reviews_Clean.txt"

# ✅ قراءة الملف النصي
with open(file_path, "r", encoding="utf-8") as file:
    text = file.read().lower()  # تحويل النص إلى lowercase

# ✅ تقسيم النص إلى كلمات (`Tokenization`)
words = word_tokenize(text)

# ✅ إزالة stop words
filtered_words = [word for word in words if word not in stop_words]

# ✅ تحويل وسوم الكلمات لـ WordNet حتى يكون Lemmatization أدق
def get_wordnet_pos(word):
    """تحويل وسوم POS الخاصة بـ NLTK إلى تنسيق WordNet"""
    from nltk.tag import pos_tag
    tag = pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ, "N": wordnet.NOUN, "V": wordnet.VERB, "R": wordnet.ADV}
    return tag_dict.get(tag, wordnet.NOUN)  # الافتراضي: NOUN

# ✅ تطبيق Lemmatization
lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in filtered_words]

# ✅ تطبيق Stemming بعد Lemmatization
stemmed_words = [stemmer.stem(word) for word in lemmatized_words]

# ✅ إعادة تجميع النص بعد Lemmatization و Stemming
lemmatized_text = " ".join(lemmatized_words)
stemmed_text = " ".join(stemmed_words)

# ✅ طباعة النص بعد Lemmatization و Stemming
print("\n✅ Text after Lemmatization \n", lemmatized_text)
print("\n✅ Text after Stemming \n", stemmed_text)

#Zainab
!pip install nltk
import nltk

# Download the required resource
nltk.download('averaged_perceptron_tagger_eng')

import nltk
from nltk.corpus import stopwords, wordnet
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer, PorterStemmer

# ✅ تحميل الموارد المطلوبة من NLTK
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

# Download 'punkt_tab' for sentence tokenization:
nltk.download('punkt_tab') # This line is added to download the required data package.

# ✅ تهيئة أدوات Lemmatization و Stemming
lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()

# ✅ تحميل قائمة stop words
stop_words = set(stopwords.words('english'))

# ✅ تحديد مسار ملف Keeta.txt
file_path = "/content/Marks_&_Spencer_Reviews_Clean.txt"

# ✅ قراءة الملف النصي
with open(file_path, "r", encoding="utf-8") as file:
    text = file.read().lower()  # تحويل النص إلى lowercase

# ✅ تقسيم النص إلى كلمات (`Tokenization`)
words = word_tokenize(text)

# ✅ إزالة stop words
filtered_words = [word for word in words if word not in stop_words]

# ✅ تحويل وسوم الكلمات لـ WordNet حتى يكون Lemmatization أدق
def get_wordnet_pos(word):
    """تحويل وسوم POS الخاصة بـ NLTK إلى تنسيق WordNet"""
    from nltk.tag import pos_tag
    tag = pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ, "N": wordnet.NOUN, "V": wordnet.VERB, "R": wordnet.ADV}
    return tag_dict.get(tag, wordnet.NOUN)  # الافتراضي: NOUN

# ✅ تطبيق Lemmatization
lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in filtered_words]

# ✅ تطبيق Stemming بعد Lemmatization
stemmed_words = [stemmer.stem(word) for word in lemmatized_words]

# ✅ إعادة تجميع النص بعد Lemmatization و Stemming
lemmatized_text = " ".join(lemmatized_words)
stemmed_text = " ".join(stemmed_words)

# ✅ طباعة النص بعد Lemmatization و Stemming
print("\n✅ Text after Lemmatization \n", lemmatized_text)
print("\n✅ Text after Stemming \n", stemmed_text)

#Zainab
!pip install nltk
import nltk

# Download the required resource
nltk.download('averaged_perceptron_tagger_eng')

import nltk
from nltk.corpus import stopwords, wordnet
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer, PorterStemmer

# ✅ تحميل الموارد المطلوبة من NLTK
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')
# Download 'punkt_tab' for sentence tokenization:
nltk.download('punkt_tab') # This line is added to download the required data package.


# ✅ تهيئة أدوات Lemmatization و Stemming
lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()

# ✅ تحميل قائمة stop words
stop_words = set(stopwords.words('english'))

# ✅ تحديد مسار ملف Keeta.txt
file_path = "/content/Stradivarius_Reviews_Clean.txt"

# ✅ قراءة الملف النصي
with open(file_path, "r", encoding="utf-8") as file:
    text = file.read().lower()  # تحويل النص إلى lowercase

# ✅ تقسيم النص إلى كلمات (`Tokenization`)
words = word_tokenize(text)

# ✅ إزالة stop words
filtered_words = [word for word in words if word not in stop_words]

# ✅ تحويل وسوم الكلمات لـ WordNet حتى يكون Lemmatization أدق
def get_wordnet_pos(word):
    """تحويل وسوم POS الخاصة بـ NLTK إلى تنسيق WordNet"""
    from nltk.tag import pos_tag
    tag = pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ, "N": wordnet.NOUN, "V": wordnet.VERB, "R": wordnet.ADV}
    return tag_dict.get(tag, wordnet.NOUN)  # الافتراضي: NOUN

# ✅ تطبيق Lemmatization
lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in filtered_words]

# ✅ تطبيق Stemming بعد Lemmatization
stemmed_words = [stemmer.stem(word) for word in lemmatized_words]

# ✅ إعادة تجميع النص بعد Lemmatization و Stemming
lemmatized_text = " ".join(lemmatized_words)
stemmed_text = " ".join(stemmed_words)

# ✅ طباعة النص بعد Lemmatization و Stemming
print("\n✅ Text after Lemmatization \n", lemmatized_text)
print("\n✅ Text after Stemming \n", stemmed_text)

#Zainab
!pip install nltk
import nltk

# Download the required resource
nltk.download('averaged_perceptron_tagger_eng')

import nltk
from nltk.corpus import stopwords, wordnet
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer, PorterStemmer

# ✅ تحميل الموارد المطلوبة من NLTK
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')
# Download 'punkt_tab' for sentence tokenization:
nltk.download('punkt_tab') # This line is added to download the required data package.


# ✅ تهيئة أدوات Lemmatization و Stemming
lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()

# ✅ تحميل قائمة stop words
stop_words = set(stopwords.words('english'))

# ✅ تحديد مسار ملف Keeta.txt
file_path = "/content/Victorias_Secret_Reviews_Clean.txt"

# ✅ قراءة الملف النصي
with open(file_path, "r", encoding="utf-8") as file:
    text = file.read().lower()  # تحويل النص إلى lowercase

# ✅ تقسيم النص إلى كلمات (`Tokenization`)
words = word_tokenize(text)

# ✅ إزالة stop words
filtered_words = [word for word in words if word not in stop_words]

# ✅ تحويل وسوم الكلمات لـ WordNet حتى يكون Lemmatization أدق
def get_wordnet_pos(word):
    """تحويل وسوم POS الخاصة بـ NLTK إلى تنسيق WordNet"""
    from nltk.tag import pos_tag
    tag = pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ, "N": wordnet.NOUN, "V": wordnet.VERB, "R": wordnet.ADV}
    return tag_dict.get(tag, wordnet.NOUN)  # الافتراضي: NOUN

# ✅ تطبيق Lemmatization
lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in filtered_words]

# ✅ تطبيق Stemming بعد Lemmatization
stemmed_words = [stemmer.stem(word) for word in lemmatized_words]

# ✅ إعادة تجميع النص بعد Lemmatization و Stemming
lemmatized_text = " ".join(lemmatized_words)
stemmed_text = " ".join(stemmed_words)

# ✅ طباعة النص بعد Lemmatization و Stemming
print("\n✅ Text after Lemmatization \n", lemmatized_text)
print("\n✅ Text after Stemming \n", stemmed_text)

#Zainab
!pip install nltk
import nltk

# Download the required resource
nltk.download('averaged_perceptron_tagger_eng')

import nltk
from nltk.corpus import stopwords, wordnet
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer, PorterStemmer

# ✅ تحميل الموارد المطلوبة من NLTK
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')
# Download 'punkt_tab' for sentence tokenization:
nltk.download('punkt_tab') # This line is added to download the required data package.


# ✅ تهيئة أدوات Lemmatization و Stemming
lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()

# ✅ تحميل قائمة stop words
stop_words = set(stopwords.words('english'))

# ✅ تحديد مسار ملف Keeta.txt
file_path = "/content/American_Eagle_Reviews_Clean.txt"

# ✅ قراءة الملف النصي
with open(file_path, "r", encoding="utf-8") as file:
    text = file.read().lower()  # تحويل النص إلى lowercase

# ✅ تقسيم النص إلى كلمات (`Tokenization`)
words = word_tokenize(text)

# ✅ إزالة stop words
filtered_words = [word for word in words if word not in stop_words]

# ✅ تحويل وسوم الكلمات لـ WordNet حتى يكون Lemmatization أدق
def get_wordnet_pos(word):
    """تحويل وسوم POS الخاصة بـ NLTK إلى تنسيق WordNet"""
    from nltk.tag import pos_tag
    tag = pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ, "N": wordnet.NOUN, "V": wordnet.VERB, "R": wordnet.ADV}
    return tag_dict.get(tag, wordnet.NOUN)  # الافتراضي: NOUN

# ✅ تطبيق Lemmatization
lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in filtered_words]

# ✅ تطبيق Stemming بعد Lemmatization
stemmed_words = [stemmer.stem(word) for word in lemmatized_words]

# ✅ إعادة تجميع النص بعد Lemmatization و Stemming
lemmatized_text = " ".join(lemmatized_words)
stemmed_text = " ".join(stemmed_words)

# ✅ طباعة النص بعد Lemmatization و Stemming
print("\n✅ Text after Lemmatization \n", lemmatized_text)
print("\n✅ Text after Stemming \n", stemmed_text)

#Hoor

!pip install nltk
import nltk

# Download the required resource
nltk.download('averaged_perceptron_tagger_eng')

import nltk
from nltk.corpus import stopwords, wordnet
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer, PorterStemmer

# ✅ تحميل الموارد المطلوبة من NLTK
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

# Download 'punkt_tab' for sentence tokenization:
nltk.download('punkt_tab') # This line is added to download the required data package.

# ✅ تهيئة أدوات Lemmatization و Stemming
lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()

# ✅ تحميل قائمة stop words
stop_words = set(stopwords.words('english'))

# ✅ تحديد مسار ملف Keeta.txt
file_path = "/content/Document ASOS.txt"

# ✅ قراءة الملف النصي
with open(file_path, "r", encoding="utf-8") as file:
    text = file.read().lower()  # تحويل النص إلى lowercase

# ✅ تقسيم النص إلى كلمات (`Tokenization`)
words = word_tokenize(text)

# ✅ إزالة stop words
filtered_words = [word for word in words if word not in stop_words]

# ✅ تحويل وسوم الكلمات لـ WordNet حتى يكون Lemmatization أدق
def get_wordnet_pos(word):
    """تحويل وسوم POS الخاصة بـ NLTK إلى تنسيق WordNet"""
    from nltk.tag import pos_tag
    tag = pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ, "N": wordnet.NOUN, "V": wordnet.VERB, "R": wordnet.ADV}
    return tag_dict.get(tag, wordnet.NOUN)  # الافتراضي: NOUN

# ✅ تطبيق Lemmatization
lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in filtered_words]

# ✅ تطبيق Stemming بعد Lemmatization
stemmed_words = [stemmer.stem(word) for word in lemmatized_words]

# ✅ إعادة تجميع النص بعد Lemmatization و Stemming
lemmatized_text = " ".join(lemmatized_words)
stemmed_text = " ".join(stemmed_words)

# ✅ طباعة النص بعد Lemmatization و Stemming
print("\n✅ Text after Lemmatization \n", lemmatized_text)
print("\n✅ Text after Stemming \n", stemmed_text)

#Hoor

!pip install nltk
import nltk

# Download the required resource
nltk.download('averaged_perceptron_tagger_eng')

import nltk
from nltk.corpus import stopwords, wordnet
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer, PorterStemmer

# ✅ تحميل الموارد المطلوبة من NLTK
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

# Download 'punkt_tab' for sentence tokenization:
nltk.download('punkt_tab') # This line is added to download the required data package.

# ✅ تهيئة أدوات Lemmatization و Stemming
lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()

# ✅ تحميل قائمة stop words
stop_words = set(stopwords.words('english'))

# ✅ تحديد مسار ملف Keeta.txt
file_path = "/content/Document Mango.txt"

# ✅ قراءة الملف النصي
with open(file_path, "r", encoding="utf-8") as file:
    text = file.read().lower()  # تحويل النص إلى lowercase

# ✅ تقسيم النص إلى كلمات (`Tokenization`)
words = word_tokenize(text)

# ✅ إزالة stop words
filtered_words = [word for word in words if word not in stop_words]

# ✅ تحويل وسوم الكلمات لـ WordNet حتى يكون Lemmatization أدق
def get_wordnet_pos(word):
    """تحويل وسوم POS الخاصة بـ NLTK إلى تنسيق WordNet"""
    from nltk.tag import pos_tag
    tag = pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ, "N": wordnet.NOUN, "V": wordnet.VERB, "R": wordnet.ADV}
    return tag_dict.get(tag, wordnet.NOUN)  # الافتراضي: NOUN

# ✅ تطبيق Lemmatization
lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in filtered_words]

# ✅ تطبيق Stemming بعد Lemmatization
stemmed_words = [stemmer.stem(word) for word in lemmatized_words]

# ✅ إعادة تجميع النص بعد Lemmatization و Stemming
lemmatized_text = " ".join(lemmatized_words)
stemmed_text = " ".join(stemmed_words)

# ✅ طباعة النص بعد Lemmatization و Stemming
print("\n✅ Text after Lemmatization \n", lemmatized_text)
print("\n✅ Text after Stemming \n", stemmed_text)

#Hoor

!pip install nltk
import nltk

# Download the required resource
nltk.download('averaged_perceptron_tagger_eng')

import nltk
from nltk.corpus import stopwords, wordnet
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer, PorterStemmer

# ✅ تحميل الموارد المطلوبة من NLTK
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

# Download 'punkt_tab' for sentence tokenization:
nltk.download('punkt_tab') # This line is added to download the required data package.

# ✅ تهيئة أدوات Lemmatization و Stemming
lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()

# ✅ تحميل قائمة stop words
stop_words = set(stopwords.words('english'))

# ✅ تحديد مسار ملف Keeta.txt
file_path = "/content/Document Bershka.txt"

# ✅ قراءة الملف النصي
with open(file_path, "r", encoding="utf-8") as file:
    text = file.read().lower()  # تحويل النص إلى lowercase

# ✅ تقسيم النص إلى كلمات (`Tokenization`)
words = word_tokenize(text)

# ✅ إزالة stop words
filtered_words = [word for word in words if word not in stop_words]

# ✅ تحويل وسوم الكلمات لـ WordNet حتى يكون Lemmatization أدق
def get_wordnet_pos(word):
    """تحويل وسوم POS الخاصة بـ NLTK إلى تنسيق WordNet"""
    from nltk.tag import pos_tag
    tag = pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ, "N": wordnet.NOUN, "V": wordnet.VERB, "R": wordnet.ADV}
    return tag_dict.get(tag, wordnet.NOUN)  # الافتراضي: NOUN

# ✅ تطبيق Lemmatization
lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in filtered_words]

# ✅ تطبيق Stemming بعد Lemmatization
stemmed_words = [stemmer.stem(word) for word in lemmatized_words]

# ✅ إعادة تجميع النص بعد Lemmatization و Stemming
lemmatized_text = " ".join(lemmatized_words)
stemmed_text = " ".join(stemmed_words)

# ✅ طباعة النص بعد Lemmatization و Stemming
print("\n✅ Text after Lemmatization \n", lemmatized_text)
print("\n✅ Text after Stemming \n", stemmed_text)

#Fatima
!pip install nltk
import nltk

# Download the required resource
nltk.download('averaged_perceptron_tagger_eng')

import nltk
from nltk.corpus import stopwords, wordnet
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer, PorterStemmer

# ✅ تحميل الموارد المطلوبة من NLTK
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

# Download 'punkt_tab' for sentence tokenization:
nltk.download('punkt_tab') # This line is added to download the required data package.

# ✅ تهيئة أدوات Lemmatization و Stemming
lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()

# ✅ تحميل قائمة stop words
stop_words = set(stopwords.words('english'))

# ✅ تحديد مسار ملف Keeta.txt
file_path = "/content/Document SHEIN.txt"

# ✅ قراءة الملف النصي
with open(file_path, "r", encoding="utf-8") as file:
    text = file.read().lower()  # تحويل النص إلى lowercase

# ✅ تقسيم النص إلى كلمات (`Tokenization`)
words = word_tokenize(text)

# ✅ إزالة stop words
filtered_words = [word for word in words if word not in stop_words]

# ✅ تحويل وسوم الكلمات لـ WordNet حتى يكون Lemmatization أدق
def get_wordnet_pos(word):
    """تحويل وسوم POS الخاصة بـ NLTK إلى تنسيق WordNet"""
    from nltk.tag import pos_tag
    tag = pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ, "N": wordnet.NOUN, "V": wordnet.VERB, "R": wordnet.ADV}
    return tag_dict.get(tag, wordnet.NOUN)  # الافتراضي: NOUN

# ✅ تطبيق Lemmatization
lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in filtered_words]

# ✅ تطبيق Stemming بعد Lemmatization
stemmed_words = [stemmer.stem(word) for word in lemmatized_words]

# ✅ إعادة تجميع النص بعد Lemmatization و Stemming
lemmatized_text = " ".join(lemmatized_words)
stemmed_text = " ".join(stemmed_words)

# ✅ طباعة النص بعد Lemmatization و Stemming
print("\n✅ Text after Lemmatization \n", lemmatized_text)
print("\n✅ Text after Stemming \n", stemmed_text)

# ✅ Sana - Text Processing with NLTK

!pip install nltk
import nltk

# تحميل الموارد المطلوبة من NLTK
nltk.download('averaged_perceptron_tagger_eng')
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')
nltk.download('punkt_tab')  # أداة تقسيم الجمل

#Hoor
!pip install nltk
import nltk


# Download the required resource
nltk.download('averaged_perceptron_tagger_eng')

import nltk
from nltk.corpus import stopwords, wordnet
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer, PorterStemmer

# ✅ تحميل الموارد المطلوبة من NLTK
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')
# Download 'punkt_tab' for sentence tokenization:
nltk.download('punkt_tab') # This line is added to download the required data package.


# ✅ تهيئة أدوات Lemmatization و Stemming
lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()

# ✅ تحميل قائمة stop words
stop_words = set(stopwords.words('english'))

# ✅ تحديد مسار ملف Keeta.txt
file_path = "/content/Tamm_Reviews.txt"

# ✅ قراءة الملف النصي
with open(file_path, "r", encoding="utf-8") as file:
    text = file.read().lower()  # تحويل النص إلى lowercase

# ✅ تقسيم النص إلى كلمات (`Tokenization`)
words = word_tokenize(text)

# ✅ إزالة stop words
filtered_words = [word for word in words if word not in stop_words]

# ✅ تحويل وسوم الكلمات لـ WordNet حتى يكون Lemmatization أدق
def get_wordnet_pos(word):
    """تحويل وسوم POS الخاصة بـ NLTK إلى تنسيق WordNet"""
    from nltk.tag import pos_tag
    tag = pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ, "N": wordnet.NOUN, "V": wordnet.VERB, "R": wordnet.ADV}
    return tag_dict.get(tag, wordnet.NOUN)  # الافتراضي: NOUN

# ✅ تطبيق Lemmatization
lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in filtered_words]

# ✅ تطبيق Stemming بعد Lemmatization
stemmed_words = [stemmer.stem(word) for word in lemmatized_words]

# ✅ إعادة تجميع النص بعد Lemmatization و Stemming
lemmatized_text = " ".join(lemmatized_words)
stemmed_text = " ".join(stemmed_words)

# ✅ طباعة النص بعد Lemmatization و Stemming
print("\n✅ Text after Lemmatization \n", lemmatized_text)
print("\n✅ Text after Stemming \n", stemmed_text)

#sana
#Shahad
!pip install nltk
import nltk

# Download the required resource
nltk.download('averaged_perceptron_tagger_eng')

import nltk
from nltk.corpus import stopwords, wordnet
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer, PorterStemmer

# ✅ تحميل الموارد المطلوبة من NLTK
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

# Download 'punkt_tab' for sentence tokenization:
nltk.download('punkt_tab') # This line is added to download the required data package.

# ✅ تهيئة أدوات Lemmatization و Stemming
lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()

# ✅ تحميل قائمة stop words
stop_words = set(stopwords.words('english'))

# ✅ تحديد مسار ملف Keeta.txt
file_path = "/content/wasl_reviews.txt"

# ✅ قراءة الملف النصي
with open(file_path, "r", encoding="utf-8") as file:
    text = file.read().lower()  # تحويل النص إلى lowercase

# ✅ تقسيم النص إلى كلمات (`Tokenization`)
words = word_tokenize(text)

# ✅ إزالة stop words
filtered_words = [word for word in words if word not in stop_words]

# ✅ تحويل وسوم الكلمات لـ WordNet حتى يكون Lemmatization أدق
def get_wordnet_pos(word):
    """تحويل وسوم POS الخاصة بـ NLTK إلى تنسيق WordNet"""
    from nltk.tag import pos_tag
    tag = pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ, "N": wordnet.NOUN, "V": wordnet.VERB, "R": wordnet.ADV}
    return tag_dict.get(tag, wordnet.NOUN)  # الافتراضي: NOUN

# ✅ تطبيق Lemmatization
lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in filtered_words]

# ✅ تطبيق Stemming بعد Lemmatization
stemmed_words = [stemmer.stem(word) for word in lemmatized_words]

# ✅ إعادة تجميع النص بعد Lemmatization و Stemming
lemmatized_text = " ".join(lemmatized_words)
stemmed_text = " ".join(stemmed_words)

# ✅ طباعة النص بعد Lemmatization و Stemming
print("\n✅ Text after Lemmatization \n", lemmatized_text)
print("\n✅ Text after Stemming \n", stemmed_text)

#sana
#Shahad
!pip install nltk
import nltk

# Download the required resource
nltk.download('averaged_perceptron_tagger_eng')

import nltk
from nltk.corpus import stopwords, wordnet
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer, PorterStemmer

# ✅ تحميل الموارد المطلوبة من NLTK
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

# Download 'punkt_tab' for sentence tokenization:
nltk.download('punkt_tab') # This line is added to download the required data package.

# ✅ تهيئة أدوات Lemmatization و Stemming
lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()

# ✅ تحميل قائمة stop words
stop_words = set(stopwords.words('english'))

# ✅ تحديد مسار ملف Keeta.txt
file_path = "/content/careemfood_reviews.txt"

# ✅ قراءة الملف النصي
with open(file_path, "r", encoding="utf-8") as file:
    text = file.read().lower()  # تحويل النص إلى lowercase

# ✅ تقسيم النص إلى كلمات (`Tokenization`)
words = word_tokenize(text)

# ✅ إزالة stop words
filtered_words = [word for word in words if word not in stop_words]

# ✅ تحويل وسوم الكلمات لـ WordNet حتى يكون Lemmatization أدق
def get_wordnet_pos(word):
    """تحويل وسوم POS الخاصة بـ NLTK إلى تنسيق WordNet"""
    from nltk.tag import pos_tag
    tag = pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ, "N": wordnet.NOUN, "V": wordnet.VERB, "R": wordnet.ADV}
    return tag_dict.get(tag, wordnet.NOUN)  # الافتراضي: NOUN

# ✅ تطبيق Lemmatization
lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in filtered_words]

# ✅ تطبيق Stemming بعد Lemmatization
stemmed_words = [stemmer.stem(word) for word in lemmatized_words]

# ✅ إعادة تجميع النص بعد Lemmatization و Stemming
lemmatized_text = " ".join(lemmatized_words)
stemmed_text = " ".join(stemmed_words)

# ✅ طباعة النص بعد Lemmatization و Stemming
print("\n✅ Text after Lemmatization \n", lemmatized_text)
print("\n✅ Text after Stemming \n", stemmed_text)

#sana
#Shahad
!pip install nltk
import nltk

# Download the required resource
nltk.download('averaged_perceptron_tagger_eng')

import nltk
from nltk.corpus import stopwords, wordnet
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer, PorterStemmer

# ✅ تحميل الموارد المطلوبة من NLTK
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

# Download 'punkt_tab' for sentence tokenization:
nltk.download('punkt_tab') # This line is added to download the required data package.

# ✅ تهيئة أدوات Lemmatization و Stemming
lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()

# ✅ تحميل قائمة stop words
stop_words = set(stopwords.words('english'))

# ✅ تحديد مسار ملف Keeta.txt
file_path = "/content/careemfood_reviews.txt"

# ✅ قراءة الملف النصي
with open(file_path, "r", encoding="utf-8") as file:
    text = file.read().lower()  # تحويل النص إلى lowercase

# ✅ تقسيم النص إلى كلمات (`Tokenization`)
words = word_tokenize(text)

# ✅ إزالة stop words
filtered_words = [word for word in words if word not in stop_words]

# ✅ تحويل وسوم الكلمات لـ WordNet حتى يكون Lemmatization أدق
def get_wordnet_pos(word):
    """تحويل وسوم POS الخاصة بـ NLTK إلى تنسيق WordNet"""
    from nltk.tag import pos_tag
    tag = pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ, "N": wordnet.NOUN, "V": wordnet.VERB, "R": wordnet.ADV}
    return tag_dict.get(tag, wordnet.NOUN)  # الافتراضي: NOUN

# ✅ تطبيق Lemmatization
lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in filtered_words]

# ✅ تطبيق Stemming بعد Lemmatization
stemmed_words = [stemmer.stem(word) for word in lemmatized_words]

# ✅ إعادة تجميع النص بعد Lemmatization و Stemming
lemmatized_text = " ".join(lemmatized_words)
stemmed_text = " ".join(stemmed_words)

# ✅ طباعة النص بعد Lemmatization و Stemming
print("\n✅ Text after Lemmatization \n", lemmatized_text)
print("\n✅ Text after Stemming \n", stemmed_text)

#sana
#Shahad
!pip install nltk
import nltk

# Download the required resource
nltk.download('averaged_perceptron_tagger_eng')

import nltk
from nltk.corpus import stopwords, wordnet
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer, PorterStemmer

# ✅ تحميل الموارد المطلوبة من NLTK
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

# Download 'punkt_tab' for sentence tokenization:
nltk.download('punkt_tab') # This line is added to download the required data package.

# ✅ تهيئة أدوات Lemmatization و Stemming
lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()

# ✅ تحميل قائمة stop words
stop_words = set(stopwords.words('english'))

# ✅ تحديد مسار ملف Keeta.txt
file_path = "/content/shaqardi_reviews.txt"

# ✅ قراءة الملف النصي
with open(file_path, "r", encoding="utf-8") as file:
    text = file.read().lower()  # تحويل النص إلى lowercase

# ✅ تقسيم النص إلى كلمات (`Tokenization`)
words = word_tokenize(text)

# ✅ إزالة stop words
filtered_words = [word for word in words if word not in stop_words]

# ✅ تحويل وسوم الكلمات لـ WordNet حتى يكون Lemmatization أدق
def get_wordnet_pos(word):
    """تحويل وسوم POS الخاصة بـ NLTK إلى تنسيق WordNet"""
    from nltk.tag import pos_tag
    tag = pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ, "N": wordnet.NOUN, "V": wordnet.VERB, "R": wordnet.ADV}
    return tag_dict.get(tag, wordnet.NOUN)  # الافتراضي: NOUN

# ✅ تطبيق Lemmatization
lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in filtered_words]

# ✅ تطبيق Stemming بعد Lemmatization
stemmed_words = [stemmer.stem(word) for word in lemmatized_words]

# ✅ إعادة تجميع النص بعد Lemmatization و Stemming
lemmatized_text = " ".join(lemmatized_words)
stemmed_text = " ".join(stemmed_words)

# ✅ طباعة النص بعد Lemmatization و Stemming
print("\n✅ Text after Lemmatization \n", lemmatized_text)
print("\n✅ Text after Stemming \n", stemmed_text)

#Hoor
!pip install nltk
import nltk


# Download the required resource
nltk.download('averaged_perceptron_tagger_eng')

import nltk
from nltk.corpus import stopwords, wordnet
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer, PorterStemmer

# ✅ تحميل الموارد المطلوبة من NLTK
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')
# Download 'punkt_tab' for sentence tokenization:
nltk.download('punkt_tab') # This line is added to download the required data package.


# ✅ تهيئة أدوات Lemmatization و Stemming
lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()

# ✅ تحميل قائمة stop words
stop_words = set(stopwords.words('english'))

# ✅ تحديد مسار ملف Keeta.txt
file_path = "/content/shaqardi_reviews.txt"

# ✅ قراءة الملف النصي
with open(file_path, "r", encoding="utf-8") as file:
    text = file.read().lower()  # تحويل النص إلى lowercase

# ✅ تقسيم النص إلى كلمات (`Tokenization`)
words = word_tokenize(text)

# ✅ إزالة stop words
filtered_words = [word for word in words if word not in stop_words]

# ✅ تحويل وسوم الكلمات لـ WordNet حتى يكون Lemmatization أدق
def get_wordnet_pos(word):
    """تحويل وسوم POS الخاصة بـ NLTK إلى تنسيق WordNet"""
    from nltk.tag import pos_tag
    tag = pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ, "N": wordnet.NOUN, "V": wordnet.VERB, "R": wordnet.ADV}
    return tag_dict.get(tag, wordnet.NOUN)  # الافتراضي: NOUN

# ✅ تطبيق Lemmatization
lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in filtered_words]

# ✅ تطبيق Stemming بعد Lemmatization
stemmed_words = [stemmer.stem(word) for word in lemmatized_words]

# ✅ إعادة تجميع النص بعد Lemmatization و Stemming
lemmatized_text = " ".join(lemmatized_words)
stemmed_text = " ".join(stemmed_words)

# ✅ طباعة النص بعد Lemmatization و Stemming
print("\n✅ Text after Lemmatization \n", lemmatized_text)
print("\n✅ Text after Stemming \n", stemmed_text)

#Hoor
!pip install nltk
import nltk


# Download the required resource
nltk.download('averaged_perceptron_tagger_eng')

import nltk
from nltk.corpus import stopwords, wordnet
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer, PorterStemmer

# ✅ تحميل الموارد المطلوبة من NLTK
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')
# Download 'punkt_tab' for sentence tokenization:
nltk.download('punkt_tab') # This line is added to download the required data package.


# ✅ تهيئة أدوات Lemmatization و Stemming
lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()

# ✅ تحميل قائمة stop words
stop_words = set(stopwords.words('english'))

# ✅ تحديد مسار ملف Keeta.txt
file_path = "/content/Document Zara.txt"

# ✅ قراءة الملف النصي
with open(file_path, "r", encoding="utf-8") as file:
    text = file.read().lower()  # تحويل النص إلى lowercase

# ✅ تقسيم النص إلى كلمات (`Tokenization`)
words = word_tokenize(text)

# ✅ إزالة stop words
filtered_words = [word for word in words if word not in stop_words]

# ✅ تحويل وسوم الكلمات لـ WordNet حتى يكون Lemmatization أدق
def get_wordnet_pos(word):
    """تحويل وسوم POS الخاصة بـ NLTK إلى تنسيق WordNet"""
    from nltk.tag import pos_tag
    tag = pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ, "N": wordnet.NOUN, "V": wordnet.VERB, "R": wordnet.ADV}
    return tag_dict.get(tag, wordnet.NOUN)  # الافتراضي: NOUN

# ✅ تطبيق Lemmatization
lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in filtered_words]

# ✅ تطبيق Stemming بعد Lemmatization
stemmed_words = [stemmer.stem(word) for word in lemmatized_words]

# ✅ إعادة تجميع النص بعد Lemmatization و Stemming
lemmatized_text = " ".join(lemmatized_words)
stemmed_text = " ".join(stemmed_words)

# ✅ طباعة النص بعد Lemmatization و Stemming
print("\n✅ Text after Lemmatization \n", lemmatized_text)
print("\n✅ Text after Stemming \n", stemmed_text)

#Fatima
!pip install nltk
import nltk

# Download the required resource
nltk.download('averaged_perceptron_tagger_eng')

import nltk
from nltk.corpus import stopwords, wordnet
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer, PorterStemmer

# ✅ تحميل الموارد المطلوبة من NLTK
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

# Download 'punkt_tab' for sentence tokenization:
nltk.download('punkt_tab') # This line is added to download the required data package.

# ✅ تهيئة أدوات Lemmatization و Stemming
lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()

# ✅ تحميل قائمة stop words
stop_words = set(stopwords.words('english'))

# ✅ تحديد مسار ملف Keeta.txt
file_path = "/content/Adidas.txt"

# ✅ قراءة الملف النصي
with open(file_path, "r", encoding="utf-8") as file:
    text = file.read().lower()  # تحويل النص إلى lowercase

# ✅ تقسيم النص إلى كلمات (`Tokenization`)
words = word_tokenize(text)

# ✅ إزالة stop words
filtered_words = [word for word in words if word not in stop_words]

# ✅ تحويل وسوم الكلمات لـ WordNet حتى يكون Lemmatization أدق
def get_wordnet_pos(word):
    """تحويل وسوم POS الخاصة بـ NLTK إلى تنسيق WordNet"""
    from nltk.tag import pos_tag
    tag = pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ, "N": wordnet.NOUN, "V": wordnet.VERB, "R": wordnet.ADV}
    return tag_dict.get(tag, wordnet.NOUN)  # الافتراضي: NOUN

# ✅ تطبيق Lemmatization
lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in filtered_words]

# ✅ تطبيق Stemming بعد Lemmatization
stemmed_words = [stemmer.stem(word) for word in lemmatized_words]

# ✅ إعادة تجميع النص بعد Lemmatization و Stemming
lemmatized_text = " ".join(lemmatized_words)
stemmed_text = " ".join(stemmed_words)

# ✅ طباعة النص بعد Lemmatization و Stemming
print("\n✅ Text after Lemmatization \n", lemmatized_text)
print("\n✅ Text after Stemming \n", stemmed_text)

#Fatima
!pip install nltk
import nltk

# Download the required resource
nltk.download('averaged_perceptron_tagger_eng')

import nltk
from nltk.corpus import stopwords, wordnet
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer, PorterStemmer

# ✅ تحميل الموارد المطلوبة من NLTK
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

# Download 'punkt_tab' for sentence tokenization:
nltk.download('punkt_tab') # This line is added to download the required data package.

# ✅ تهيئة أدوات Lemmatization و Stemming
lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()

# ✅ تحميل قائمة stop words
stop_words = set(stopwords.words('english'))

# ✅ تحديد مسار ملف Keeta.txt
file_path = "/content/To You.txt"

# ✅ قراءة الملف النصي
with open(file_path, "r", encoding="utf-8") as file:
    text = file.read().lower()  # تحويل النص إلى lowercase

# ✅ تقسيم النص إلى كلمات (`Tokenization`)
words = word_tokenize(text)

# ✅ إزالة stop words
filtered_words = [word for word in words if word not in stop_words]

# ✅ تحويل وسوم الكلمات لـ WordNet حتى يكون Lemmatization أدق
def get_wordnet_pos(word):
    """تحويل وسوم POS الخاصة بـ NLTK إلى تنسيق WordNet"""
    from nltk.tag import pos_tag
    tag = pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ, "N": wordnet.NOUN, "V": wordnet.VERB, "R": wordnet.ADV}
    return tag_dict.get(tag, wordnet.NOUN)  # الافتراضي: NOUN

# ✅ تطبيق Lemmatization
lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in filtered_words]

# ✅ تطبيق Stemming بعد Lemmatization
stemmed_words = [stemmer.stem(word) for word in lemmatized_words]

# ✅ إعادة تجميع النص بعد Lemmatization و Stemming
lemmatized_text = " ".join(lemmatized_words)
stemmed_text = " ".join(stemmed_words)

# ✅ طباعة النص بعد Lemmatization و Stemming
print("\n✅ Text after Lemmatization \n", lemmatized_text)
print("\n✅ Text after Stemming \n", stemmed_text)

#Fatima
!pip install nltk
import nltk

# Download the required resource
nltk.download('averaged_perceptron_tagger_eng')

import nltk
from nltk.corpus import stopwords, wordnet
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer, PorterStemmer

# ✅ تحميل الموارد المطلوبة من NLTK
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

# Download 'punkt_tab' for sentence tokenization:
nltk.download('punkt_tab') # This line is added to download the required data package.

# ✅ تهيئة أدوات Lemmatization و Stemming
lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()

# ✅ تحميل قائمة stop words
stop_words = set(stopwords.words('english'))

# ✅ تحديد مسار ملف Keeta.txt
file_path = "/content/Chefz.txt"

# ✅ قراءة الملف النصي
with open(file_path, "r", encoding="utf-8") as file:
    text = file.read().lower()  # تحويل النص إلى lowercase

# ✅ تقسيم النص إلى كلمات (`Tokenization`)
words = word_tokenize(text)

# ✅ إزالة stop words
filtered_words = [word for word in words if word not in stop_words]

# ✅ تحويل وسوم الكلمات لـ WordNet حتى يكون Lemmatization أدق
def get_wordnet_pos(word):
    """تحويل وسوم POS الخاصة بـ NLTK إلى تنسيق WordNet"""
    from nltk.tag import pos_tag
    tag = pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ, "N": wordnet.NOUN, "V": wordnet.VERB, "R": wordnet.ADV}
    return tag_dict.get(tag, wordnet.NOUN)  # الافتراضي: NOUN

# ✅ تطبيق Lemmatization
lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in filtered_words]

# ✅ تطبيق Stemming بعد Lemmatization
stemmed_words = [stemmer.stem(word) for word in lemmatized_words]

# ✅ إعادة تجميع النص بعد Lemmatization و Stemming
lemmatized_text = " ".join(lemmatized_words)
stemmed_text = " ".join(stemmed_words)

# ✅ طباعة النص بعد Lemmatization و Stemming
print("\n✅ Text after Lemmatization \n", lemmatized_text)
print("\n✅ Text after Stemming \n", stemmed_text)

#Fatima
!pip install nltk
import nltk

# Download the required resource
nltk.download('averaged_perceptron_tagger_eng')

import nltk
from nltk.corpus import stopwords, wordnet
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer, PorterStemmer

# ✅ تحميل الموارد المطلوبة من NLTK
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

# Download 'punkt_tab' for sentence tokenization:
nltk.download('punkt_tab') # This line is added to download the required data package.

# ✅ تهيئة أدوات Lemmatization و Stemming
lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()

# ✅ تحميل قائمة stop words
stop_words = set(stopwords.words('english'))

# ✅ تحديد مسار ملف Keeta.txt
file_path = "/content/Jenny.txt"

# ✅ قراءة الملف النصي
with open(file_path, "r", encoding="utf-8") as file:
    text = file.read().lower()  # تحويل النص إلى lowercase

# ✅ تقسيم النص إلى كلمات (`Tokenization`)
words = word_tokenize(text)

# ✅ إزالة stop words
filtered_words = [word for word in words if word not in stop_words]

# ✅ تحويل وسوم الكلمات لـ WordNet حتى يكون Lemmatization أدق
def get_wordnet_pos(word):
    """تحويل وسوم POS الخاصة بـ NLTK إلى تنسيق WordNet"""
    from nltk.tag import pos_tag
    tag = pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ, "N": wordnet.NOUN, "V": wordnet.VERB, "R": wordnet.ADV}
    return tag_dict.get(tag, wordnet.NOUN)  # الافتراضي: NOUN

# ✅ تطبيق Lemmatization
lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in filtered_words]

# ✅ تطبيق Stemming بعد Lemmatization
stemmed_words = [stemmer.stem(word) for word in lemmatized_words]

# ✅ إعادة تجميع النص بعد Lemmatization و Stemming
lemmatized_text = " ".join(lemmatized_words)
stemmed_text = " ".join(stemmed_words)

# ✅ طباعة النص بعد Lemmatization و Stemming
print("\n✅ Text after Lemmatization \n", lemmatized_text)
print("\n✅ Text after Stemming \n", stemmed_text)

#Fatima
!pip install nltk
import nltk

# Download the required resource
nltk.download('averaged_perceptron_tagger_eng')

import nltk
from nltk.corpus import stopwords, wordnet
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer, PorterStemmer

# ✅ تحميل الموارد المطلوبة من NLTK
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

# Download 'punkt_tab' for sentence tokenization:
nltk.download('punkt_tab') # This line is added to download the required data package.

# ✅ تهيئة أدوات Lemmatization و Stemming
lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()

# ✅ تحميل قائمة stop words
stop_words = set(stopwords.words('english'))

# ✅ تحديد مسار ملف Keeta.txt
file_path = "/content/Namshi.txt"

# ✅ قراءة الملف النصي
with open(file_path, "r", encoding="utf-8") as file:
    text = file.read().lower()  # تحويل النص إلى lowercase

# ✅ تقسيم النص إلى كلمات (`Tokenization`)
words = word_tokenize(text)

# ✅ إزالة stop words
filtered_words = [word for word in words if word not in stop_words]

# ✅ تحويل وسوم الكلمات لـ WordNet حتى يكون Lemmatization أدق
def get_wordnet_pos(word):
    """تحويل وسوم POS الخاصة بـ NLTK إلى تنسيق WordNet"""
    from nltk.tag import pos_tag
    tag = pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ, "N": wordnet.NOUN, "V": wordnet.VERB, "R": wordnet.ADV}
    return tag_dict.get(tag, wordnet.NOUN)  # الافتراضي: NOUN

# ✅ تطبيق Lemmatization
lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in filtered_words]

# ✅ تطبيق Stemming بعد Lemmatization
stemmed_words = [stemmer.stem(word) for word in lemmatized_words]

# ✅ إعادة تجميع النص بعد Lemmatization و Stemming
lemmatized_text = " ".join(lemmatized_words)
stemmed_text = " ".join(stemmed_words)

# ✅ طباعة النص بعد Lemmatization و Stemming
print("\n✅ Text after Lemmatization \n", lemmatized_text)
print("\n✅ Text after Stemming \n", stemmed_text)

#Fatima
!pip install nltk
import nltk

# Download the required resource
nltk.download('averaged_perceptron_tagger_eng')

import nltk
from nltk.corpus import stopwords, wordnet
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer, PorterStemmer

# ✅ تحميل الموارد المطلوبة من NLTK
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

# Download 'punkt_tab' for sentence tokenization:
nltk.download('punkt_tab') # This line is added to download the required data package.

# ✅ تهيئة أدوات Lemmatization و Stemming
lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()

# ✅ تحميل قائمة stop words
stop_words = set(stopwords.words('english'))

# ✅ تحديد مسار ملف Keeta.txt
file_path = "/content/Nike.txt"

# ✅ قراءة الملف النصي
with open(file_path, "r", encoding="utf-8") as file:
    text = file.read().lower()  # تحويل النص إلى lowercase

# ✅ تقسيم النص إلى كلمات (`Tokenization`)
words = word_tokenize(text)

# ✅ إزالة stop words
filtered_words = [word for word in words if word not in stop_words]

# ✅ تحويل وسوم الكلمات لـ WordNet حتى يكون Lemmatization أدق
def get_wordnet_pos(word):
    """تحويل وسوم POS الخاصة بـ NLTK إلى تنسيق WordNet"""
    from nltk.tag import pos_tag
    tag = pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ, "N": wordnet.NOUN, "V": wordnet.VERB, "R": wordnet.ADV}
    return tag_dict.get(tag, wordnet.NOUN)  # الافتراضي: NOUN

# ✅ تطبيق Lemmatization
lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in filtered_words]

# ✅ تطبيق Stemming بعد Lemmatization
stemmed_words = [stemmer.stem(word) for word in lemmatized_words]

# ✅ إعادة تجميع النص بعد Lemmatization و Stemming
lemmatized_text = " ".join(lemmatized_words)
stemmed_text = " ".join(stemmed_words)

# ✅ طباعة النص بعد Lemmatization و Stemming
print("\n✅ Text after Lemmatization \n", lemmatized_text)
print("\n✅ Text after Stemming \n", stemmed_text)

from sklearn.feature_extraction.text import TfidfVectorizer
import os
import glob
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# ✅ تحديد مسار جميع الملفات النصية بعد المعالجة
folder_path = "/content/"  # المسار إلى المجلد الذي يحتوي على الملفات النصية
file_paths = glob.glob(os.path.join(folder_path, "*.txt"))  # البحث عن جميع الملفات النصية

# ✅ قراءة محتوى جميع الملفات
documents = []
file_names = []

for file_path in file_paths:
    with open(file_path, "r", encoding="utf-8") as file:
        documents.append(file.read())  # حفظ محتوى الملف
        file_names.append(os.path.basename(file_path))  # حفظ اسم الملف

# ✅ تهيئة TF-IDF Vectorizer
vectorizer = TfidfVectorizer()

# ✅ تطبيق TF-IDF على جميع المستندات
tfidf_matrix = vectorizer.fit_transform(documents)

# ✅ تحويل TF-IDF Matrix إلى DataFrame
df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out(), index=file_names)

# ✅ عرض النتائج
print("\n✅ TF-IDF Matrix:\n")
print(df_tfidf)

# ✅ حفظ النتائج في ملف CSV إذا أردت الرجوع إليها لاحقًا
df_tfidf.to_csv("tfidf_result.csv", index=True)
print("\n✅ TF-IDF results saved to 'tfidf_result.csv'")


# ✅ استخدم مصفوفة TF-IDF المحسوبة مسبقًا
tfidf_matrix = df_tfidf.to_numpy()  # تحويل DataFrame إلى مصفوفة NumPy

# ✅ إدخال استعلام البحث يدويًا
query = input("Enter your search query: ").strip()

# ✅ تحويل الاستعلام إلى تمثيل TF-IDF باستخدام نفس النموذج المحسوب مسبقًا
if query:  # التحقق من أن الإدخال ليس فارغًا
    query_vector = vectorizer.transform([query]).toarray()

    # ✅ حساب التشابه باستخدام Cosine Similarity
    cosine_similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()

    # ✅ ترتيب المستندات بناءً على أعلى تشابه
    sorted_indices = np.argsort(cosine_similarities)[::-1]  # ترتيب تنازلي

    # ✅ استبعاد المستندات التي تشابهها = 0
    sorted_indices = [i for i in sorted_indices if cosine_similarities[i] > 0]

    # ✅ جلب أسماء الملفات المرتبة بناءً على التشابه
    sorted_files = [df_tfidf.index[i] for i in sorted_indices]

    # ✅ عرض المستندات المتطابقة فقط
    if sorted_files:
        print("\n✅ Retrieved Documents:")
        for i, doc in enumerate(sorted_files):
            print(f"{i+1}. {doc} (Similarity: {cosine_similarities[sorted_indices[i]]:.4f})")
    else:
        print("\n⚠️ لا يوجد مستندات متطابقة مع الاستعلام.")

    # ✅ حساب Precision و Recall إذا كان هناك مستندات مسترجعة
    if sorted_indices:
        retrieved_docs = set(sorted_indices)  # جميع المستندات المسترجعة (باستثناء التشابه 0)
        relevant_docs = set([0, 1, 3])  # 🔹 ضع هنا الفهارس الحقيقية للمستندات ذات الصلة للاستعلام

        true_positives = len(retrieved_docs & relevant_docs)
        precision = true_positives / len(retrieved_docs) if retrieved_docs else 0
        recall = true_positives / len(relevant_docs) if relevant_docs else 0

        print("\n✅ Performance Metrics:")
        print(f"Precision: {precision:.4f}")
        print(f"Recall: {recall:.4f}")
    else:
        print("\n⚠️ لا يمكن حساب Precision و Recall لأنه لم يتم استرجاع أي مستندات.")
else:
    print("\n⚠️ لم يتم إدخال استعلام بحث.")

# test
from sklearn.feature_extraction.text import TfidfVectorizer
import os
import glob
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# ✅ تحديد مسار جميع الملفات النصية بعد المعالجة
folder_path = "/content/"  # المسار إلى المجلد الذي يحتوي على الملفات النصية
file_paths = glob.glob(os.path.join(folder_path, "*.txt"))  # البحث عن جميع الملفات النصية

# ✅ قراءة محتوى جميع الملفات
documents = []
file_names = []

for file_path in file_paths:
    with open(file_path, "r", encoding="utf-8") as file:
        documents.append(file.read())  # حفظ محتوى الملف
        file_names.append(os.path.basename(file_path))  # حفظ اسم الملف

# ✅ تهيئة TF-IDF Vectorizer
vectorizer = TfidfVectorizer()

# ✅ تطبيق TF-IDF على جميع المستندات
tfidf_matrix = vectorizer.fit_transform(documents)

# ✅ تحويل TF-IDF Matrix إلى DataFrame
df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out(), index=file_names)

# ✅ عرض النتائج
print("\n✅ TF-IDF Matrix:\n")
print(df_tfidf)

# ✅ حفظ النتائج في ملف CSV إذا أردت الرجوع إليها لاحقًا
df_tfidf.to_csv("tfidf_result.csv", index=True)
print("\n✅ TF-IDF results saved to 'tfidf_result.csv'")

# ✅ استخدم مصفوفة TF-IDF المحسوبة مسبقًا
tfidf_matrix = df_tfidf.to_numpy()  # تحويل DataFrame إلى مصفوفة NumPy

# ✅ إدخال استعلام البحث يدويًا
query = input("Enter your search query: ").strip()

# ✅ تحويل الاستعلام إلى تمثيل TF-IDF باستخدام نفس النموذج المحسوب مسبقًا
if query:  # التحقق من أن الإدخال ليس فارغًا
    query_vector = vectorizer.transform([query]).toarray()

    # ✅ حساب التشابه باستخدام Cosine Similarity
    cosine_similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()

    # ✅ ترتيب المستندات بناءً على أعلى تشابه
    sorted_indices = np.argsort(cosine_similarities)[::-1]  # ترتيب تنازلي

    # ✅ استبعاد المستندات التي تشابهها = 0
    sorted_indices = [i for i in sorted_indices if cosine_similarities[i] > 0]

    # ✅ جلب أسماء الملفات المرتبة بناءً على التشابه
    sorted_files = [df_tfidf.index[i] for i in sorted_indices]

    # ✅ عرض المستندات المتطابقة فقط
    if sorted_files:
        print("\n✅ Retrieved Documents:")
        for i, doc in enumerate(sorted_files):
            print(f"{i+1}. {doc} (Similarity: {cosine_similarities[sorted_indices[i]]:.4f})")
    else:
        print("\n⚠️ لا يوجد مستندات متطابقة مع الاستعلام.")

    # ✅ حساب Precision و Recall إذا كان هناك مستندات مسترجعة
    if sorted_indices:
        retrieved_docs = set(sorted_indices)  # جميع المستندات المسترجعة (باستثناء التشابه 0)

        # 🔹 ضع هنا الفهارس الحقيقية للمستندات ذات الصلة للاستعلام
        relevant_docs = set([0, 1, 3])  # على سبيل المثال، قم بتغيير هذه الفهارس بناءً على الاستعلام

        true_positives = len(retrieved_docs & relevant_docs)
        precision = true_positives / len(retrieved_docs) if retrieved_docs else 0
        recall = true_positives / len(relevant_docs) if relevant_docs else 0

        # ✅ عرض Precision و Recall
        print("\n✅ Performance Metrics:")
        print(f"Precision: {precision:.4f}")
        print(f"Recall: {recall:.4f}")
    else:
        print("\n⚠️ لا يمكن حساب Precision و Recall لأنه لم يتم استرجاع أي مستندات.")
else:
    print("\n⚠️ لم يتم إدخال استعلام بحث.")

import pandas as pd
from sklearn.cluster import KMeans
from sklearn.metrics import accuracy_score, precision_score, recall_score
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import numpy as np

# تحميل البيانات (تأكد إنك قرأتها من ملف CSV مثل ما سوينا قبل)
df = pd.read_csv('/content/archive.zip')  # غير المسار إذا الملف مختلف

# فصل الميزات عن التسمية
X = df.drop('Outcome', axis=1)   # نستخدم فقط الميزات (بدون Outcome)
y_true = df['Outcome']           # نحفظ Outcome للمقارنة لاحقًا

# توحيد القيم (Normalization) لتحسين أداء KMeans
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# تطبيق KMeans مع 2 clusters
kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)
clusters = kmeans.fit_predict(X_scaled)

# تعديل التسمية (لأن KMeans يسمي العناقيد عشوائيًا)
# نحاول نطابق التسميات مع y_true للحصول على أفضل دقة
# إذا عكسنا التسميات نحصل على دقة أفضل؟ نشيك
accuracy = accuracy_score(y_true, clusters)
accuracy_flipped = accuracy_score(y_true, 1 - clusters)
final_accuracy = max(accuracy, accuracy_flipped)

# حساب Precision و Recall
precision = precision_score(y_true, clusters)
recall = recall_score(y_true, clusters)

# طباعة النتائج
print("دقة K-Means بعد المقارنة مع Outcome:", f"{final_accuracy * 100:.2f}%")
print("Precision:", f"{precision * 100:.2f}%")
print("Recall:", f"{recall * 100:.2f}%")

# رسم بياني (اختياري) لتوزيع أول ميزتين مثلاً
plt.figure(figsize=(8,6))
plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters, cmap='viridis', alpha=0.6)
plt.title('K-Means Clustering (عرض لأول ميزتين فقط)')
plt.xlabel('Pregnancies (Scaled)')
plt.ylabel('Glucose (Scaled)')
plt.grid(True)
plt.show()

# ربط Google Colab بالملف المرفوع
import zipfile
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report
import os
import matplotlib.pyplot as plt

# المسارات
zip_file_path = '/content/archive.zip'
extracted_folder = '/content/extracted_data'

# إنشاء مجلد الاستخراج
os.makedirs(extracted_folder, exist_ok=True)

# فك الضغط
if os.path.exists(zip_file_path):
    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
        zip_ref.extractall(extracted_folder)
        print("تم فك الضغط بنجاح. الملفات داخل ZIP:")
        print(zip_ref.namelist())
else:
    raise FileNotFoundError(f"لم يتم العثور على الملف: {zip_file_path}")

# استعراض الملفات داخل مجلد الاستخراج
print("محتويات مجلد الاستخراج:", os.listdir(extracted_folder))

# البحث عن ملف CSV
csv_files = [f for f in os.listdir(extracted_folder) if f.endswith('.csv')]
if csv_files:
    csv_file_path = os.path.join(extracted_folder, csv_files[0])
    print(f"تم استخدام ملف CSV: {csv_file_path}")
else:
    raise FileNotFoundError("لم يتم العثور على أي ملف CSV داخل المجلد المستخرج")

# قراءة البيانات
df = pd.read_csv(csv_file_path)

# تجهيز البيانات
X = df.drop('Outcome', axis=1)
y = df['Outcome']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Decision Tree Model
dt = DecisionTreeClassifier()
dt.fit(X_train, y_train)
y_pred_dt = dt.predict(X_test)

# K-Nearest Neighbors Model
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, y_train)
y_pred_knn = knn.predict(X_test)

# حساب القياسات لكل خوارزمية
# Decision Tree
accuracy_dt = accuracy_score(y_test, y_pred_dt)
precision_dt = precision_score(y_test, y_pred_dt)
recall_dt = recall_score(y_test, y_pred_dt)

# KNN
accuracy_knn = accuracy_score(y_test, y_pred_knn)
precision_knn = precision_score(y_test, y_pred_knn)
recall_knn = recall_score(y_test, y_pred_knn)

# طباعة النتائج بشكل أنيق
print("\n===== نتائج التصنيف =====")
print(f"Decision Tree Accuracy: {accuracy_dt * 100:.2f}%")
print(f"Decision Tree Precision: {precision_dt * 100:.2f}%")
print(f"Decision Tree Recall: {recall_dt * 100:.2f}%")

print(f"\nKNN Accuracy: {accuracy_knn * 100:.2f}%")
print(f"KNN Precision: {precision_knn * 100:.2f}%")
print(f"KNN Recall: {recall_knn * 100:.2f}%")

# رسم بياني للمقارنة
models = ['Decision Tree', 'KNN']
accuracies = [accuracy_dt * 100, accuracy_knn * 100]
precisions = [precision_dt * 100, precision_knn * 100]
recalls = [recall_dt * 100, recall_knn * 100]

# رسم بياني للمقارنة
plt.figure(figsize=(10, 6))
bar_width = 0.25
index = range(len(models))

plt.bar(index, accuracies, bar_width, label='Accuracy')
plt.bar([i + bar_width for i in index], precisions, bar_width, label='Precision')
plt.bar([i + 2 * bar_width for i in index], recalls, bar_width, label='Recall')

plt.xlabel('Models')
plt.ylabel('Percentage')
plt.title('Model Comparison: Accuracy, Precision, and Recall')
plt.xticks([i + bar_width for i in index], models)
plt.ylim(0, 100)

# إضافة النصوص فوق الأعمدة
for i, acc in enumerate(accuracies):
    plt.text(i, acc + 2, f"{acc:.2f}%", ha='center')

for i, prec in enumerate(precisions):
    plt.text(i + bar_width, prec + 2, f"{prec:.2f}%", ha='center')

for i, rec in enumerate(recalls):
    plt.text(i + 2 * bar_width, rec + 2, f"{rec:.2f}%", ha='center')

plt.legend()
plt.show()

print("أعمدة df الأصلية:")
print(df.columns)

print("\nأعمدة X (بدون Outcome):")
print(X.columns)

print(type(X))
X.head()

print(df.columns)  # شوف أسماء الأعمدة بعد قراءة الملف

# === [1] المكتبات والتجهيز الأساسي ===
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.cluster import KMeans
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score
from IPython.display import display, HTML, clear_output
from google.colab import output
import plotly.express as px
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
import os
import glob
from sklearn.metrics.pairwise import cosine_similarity
import ipywidgets as widgets
from IPython.display import Javascript

# === [2] تحميل البيانات ===
def load_diabetes_data():
    url = "https://raw.githubusercontent.com/plotly/datasets/master/diabetes.csv"
    return pd.read_csv(url)

# === [3] كلاس التحليل ===
class DiabetesAnalyzer:
    def __init__(self):
        self.df = load_diabetes_data()
        self.dt_model = None
        self.knn_model = None
        self.scaler = None
        self.kmeans = None

    def train_compare_models(self):
        X = self.df.drop('Outcome', axis=1)
        y = self.df['Outcome']
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

        # Decision Tree Model
        self.dt_model = DecisionTreeClassifier()
        self.dt_model.fit(X_train, y_train)
        y_pred_dt = self.dt_model.predict(X_test)

        # K-Nearest Neighbors Model
        self.knn_model = KNeighborsClassifier(n_neighbors=3)
        self.knn_model.fit(X_train, y_train)
        y_pred_knn = self.knn_model.predict(X_test)

        # حساب القياسات لكل خوارزمية
        metrics = {
            'Decision Tree': {
                'accuracy': accuracy_score(y_test, y_pred_dt),
                'precision': precision_score(y_test, y_pred_dt),
                'recall': recall_score(y_test, y_pred_dt)
            },
            'KNN': {
                'accuracy': accuracy_score(y_test, y_pred_knn),
                'precision': precision_score(y_test, y_pred_knn),
                'recall': recall_score(y_test, y_pred_knn)
            }
        }

        return metrics

    def perform_clustering(self):
        X = self.df.drop('Outcome', axis=1)
        y_true = self.df['Outcome']

        # توحيد القيم (Normalization)
        self.scaler = StandardScaler()
        X_scaled = self.scaler.fit_transform(X)

        # تطبيق KMeans مع 2 clusters
        self.kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)
        clusters = self.kmeans.fit_predict(X_scaled)

        # تعديل التسمية للحصول على أفضل دقة
        accuracy = accuracy_score(y_true, clusters)
        accuracy_flipped = accuracy_score(y_true, 1 - clusters)
        final_accuracy = max(accuracy, accuracy_flipped)

        # حساب Precision و Recall
        precision = precision_score(y_true, clusters)
        recall = recall_score(y_true, clusters)

        # إضافة النتائج للعرض
        results = {
            'accuracy': final_accuracy,
            'precision': precision,
            'recall': recall,
            'clusters': clusters
        }

        return results

    def predict_diabetes(self, input_data):
        # تحويل البيانات المدخلة إلى مصفوفة numpy
        input_array = np.array([input_data])

        # استخدام نموذج شجرة القرار للتنبؤ
        prediction = self.dt_model.predict(input_array)

        # تحويل النتيجة إلى نص مفهوم
        if prediction[0] == 1:
            return "مصاب بالسكري"
        else:
            return "غير مصاب بالسكري"

# === [4] واجهة المستخدم ===
class DiabetesUI:
    def __init__(self):
        self.analyzer = DiabetesAnalyzer()
        self.analyzer.train_compare_models()  # تدريب النماذج عند التهيئة
        self.create_interface()
        self.vectorizer = None
        self.df_tfidf = None
        self.file_names = None
        self.relevant_docs = set()  # Track relevant documents for evaluation

    def create_interface(self):
        html = """
        <style>
            .google-container {
                font-family: Arial, sans-serif;
                text-align: center;
                margin-top: 50px;
                max-width: 800px;
                margin-left: auto;
                margin-right: auto;
            }

            .google-logo {
                font-size: 60px;
                font-weight: bold;
                margin-bottom: 20px;
            }

            .G { color: #4285F4; }
            .o1 { color: #EA4335; }
            .o2 { color: #FBBC05; }
            .g { color: #34A853; }
            .l { color: #EA4335; }
            .e { color: #4285F4; }

            .search-container {
                margin: 20px auto;
                max-width: 600px;
            }

            .search-box {
                width: 100%;
                padding: 12px 20px;
                margin: 8px 0;
                border: 1px solid #dfe1e5;
                border-radius: 24px;
                font-size: 16px;
                box-shadow: 0 1px 6px rgba(32,33,36,0.28);
            }

            .search-box:focus {
                outline: none;
                box-shadow: 0 1px 6px rgba(32,33,36,0.28);
            }

            .google-btn {
                background-color: #f8f9fa;
                border: 1px solid #f8f9fa;
                border-radius: 4px;
                color: #3c4043;
                padding: 10px 20px;
                margin: 0 5px;
                font-size: 14px;
                cursor: pointer;
                transition: all 0.3s;
            }

            .google-btn:hover {
                border: 1px solid #dadce0;
                box-shadow: 0 1px 1px rgba(0,0,0,0.1);
            }

            .result-area {
                margin-top: 40px;
                padding: 20px;
                text-align: left;
                border-top: 1px solid #ebebeb;
                max-width: 800px;
                margin-left: auto;
                margin-right: auto;
            }

            .metrics-table {
                width: 100%;
                border-collapse: collapse;
                margin: 20px 0;
            }

            .metrics-table th, .metrics-table td {
                border: 1px solid #ddd;
                padding: 8px;
                text-align: center;
            }

            .metrics-table th {
                background-color: #f2f2f2;
            }

            /* New styles for prediction form */
            .input-form {
                margin: 20px 0;
                padding: 20px;
                border: 1px solid #eee;
                border-radius: 8px;
                background-color: #f9f9f9;
            }

            .input-row {
                display: flex;
                justify-content: space-between;
                margin-bottom: 10px;
            }

            .input-label {
                width: 30%;
                text-align: right;
                padding-right: 10px;
            }

            .input-field {
                width: 65%;
            }

            .predict-btn {
                background-color: #4285F4;
                color: white;
                border: none;
                padding: 10px 20px;
                border-radius: 4px;
                cursor: pointer;
                font-size: 14px;
                margin-top: 10px;
            }

            .predict-btn:hover {
                background-color: #3367D6;
            }

            .prediction-result {
                margin-top: 20px;
                padding: 15px;
                border-radius: 4px;
                font-weight: bold;
                text-align: center;
            }

            .diabetic {
                background-color: #FFCDD2;
                color: #C62828;
            }

            .non-diabetic {
                background-color: #C8E6C9;
                color: #388E3C;
            }
        </style>

        <div class="google-container">
            <div class="google-logo">
                <span class="G">G</span><span class="o1">o</span><span class="o2">o</span>
                <span class="g">g</span><span class="l">l</span><span class="e">e</span>
            </div>

            <div class="search-container">
                <input type="text" id="searchQuery" class="search-box" placeholder="Search documents...">
            </div>

            <div class="buttons">
                <button class="google-btn" onclick="google.colab.kernel.invokeFunction('notebook.showData', [], {})">Information Retrieval</button>
                <button class="google-btn" onclick="google.colab.kernel.invokeFunction('notebook.runClustering', [], {})">Clustering</button>
                <button class="google-btn" onclick="google.colab.kernel.invokeFunction('notebook.showClassificationForm', [], {})">Classification</button>
            </div>

            <div id="resultArea" class="result-area"></div>
            <div id="plotArea" style="margin-top: 20px;"></div>
        </div>
        """
        display(HTML(html))

    def show_data(self):
        clear_output()
        display(HTML("<div class='result-area'><h3>Information Retrieval System</h3></div>"))

        # Information Retrieval Code - Updated version
        folder_path = "/content/"
        file_paths = glob.glob(os.path.join(folder_path, "*.txt"))

        if not file_paths:
            display(HTML("<div class='result-area'><p>No text files found in /content/ directory.</p></div>"))
            return

        documents = []
        self.file_names = []

        for file_path in file_paths:
            with open(file_path, "r", encoding="utf-8") as file:
                documents.append(file.read())
                self.file_names.append(os.path.basename(file_path))

        # Initialize TF-IDF Vectorizer
        self.vectorizer = TfidfVectorizer()

        # Apply TF-IDF to all documents
        tfidf_matrix = self.vectorizer.fit_transform(documents)

        # Convert TF-IDF Matrix to DataFrame
        self.df_tfidf = pd.DataFrame(tfidf_matrix.toarray(),
                                   columns=self.vectorizer.get_feature_names_out(),
                                   index=self.file_names)

        display(HTML("<div class='result-area'><h4>TF-IDF Matrix:</h4></div>"))
        display(self.df_tfidf.head())

        # Save results to CSV
        self.df_tfidf.to_csv("tfidf_result.csv", index=True)
        display(HTML("<div class='result-area'><p>✅ TF-IDF results saved to 'tfidf_result.csv'</p></div>"))

        # Create search box
        search_html = """
        <div class="search-container">
            <input type="text" id="searchQuery" class="search-box" placeholder="Enter your search query...">
            <button class="google-btn" onclick="google.colab.kernel.invokeFunction('notebook.performSearch', [document.getElementById('searchQuery').value], {})"
                    style="margin-top: 10px;">Search</button>
        </div>
        """
        display(HTML(search_html))

    def perform_search(self, query):
        clear_output()
        self.show_data()  # Show the data again

        if query:
            # Convert query to TF-IDF representation
            query_vector = self.vectorizer.transform([query]).toarray()

            # Convert DataFrame to NumPy array
            tfidf_matrix = self.df_tfidf.to_numpy()

            # Calculate cosine similarity
            cosine_similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()

            # Sort documents by similarity (descending)
            sorted_indices = np.argsort(cosine_similarities)[::-1]

            # Exclude documents with 0 similarity
            sorted_indices = [i for i in sorted_indices if cosine_similarities[i] > 0]
            sorted_files = [self.df_tfidf.index[i] for i in sorted_indices]

            if sorted_files:
                result_html = "<div class='result-area'><h4>Search Results:</h4><ul>"
                for i, doc in enumerate(sorted_files):
                    result_html += f"<li>{doc} (Similarity: {cosine_similarities[sorted_indices[i]]:.4f})</li>"
                result_html += "</ul></div>"
                display(HTML(result_html))

                # Performance metrics - we'll assume top 3 results are relevant for demo purposes
                retrieved_docs = set(sorted_indices[:3])  # Consider top 3 as retrieved
                self.relevant_docs.update(retrieved_docs)  # Add these to relevant docs for future searches

                true_positives = len(retrieved_docs & self.relevant_docs)
                precision = true_positives / len(retrieved_docs) if retrieved_docs else 0
                recall = true_positives / len(self.relevant_docs) if self.relevant_docs else 0

                metrics_html = f"""
                <div class='result-area'>
                    <h4>Performance Metrics:</h4>
                    <p>Precision: {precision:.4f} (Relevant documents in top results)</p>
                    <p>Recall: {recall:.4f} (Fraction of all known relevant documents found)</p>
                    <p><small>Note: For demonstration, we consider top 3 results as retrieved and relevant.</small></p>
                </div>
                """
                display(HTML(metrics_html))
            else:
                display(HTML("<div class='result-area'><p>No matching documents found.</p></div>"))
        else:
            display(HTML("<div class='result-area'><p>Please enter a search query.</p></div>"))

    def run_clustering(self):
        clear_output()
        results = self.analyzer.perform_clustering()

        # إنشاء عرض النتائج
        html = f"""
        <div class="result-area">
            <h3>Clustering Results (K-Means)</h3>
            <table class="metrics-table">
                <tr>
                    <th>Metric</th>
                    <th>Value</th>
                </tr>
                <tr>
                    <td>Accuracy</td>
                    <td>{results['accuracy']*100:.2f}%</td>
                </tr>
                <tr>
                    <td>Precision</td>
                    <td>{results['precision']*100:.2f}%</td>
                </tr>
                <tr>
                    <td>Recall</td>
                    <td>{results['recall']*100:.2f}%</td>
                </tr>
            </table>
        </div>
        """
        display(HTML(html))

        # رسم بياني للتجميع
        X_scaled = self.analyzer.scaler.transform(self.analyzer.df.drop('Outcome', axis=1))
        plt.figure(figsize=(8,6))
        plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=results['clusters'], cmap='viridis', alpha=0.6)
        plt.title('K-Means Clustering (First Two Features)')
        plt.xlabel('Pregnancies (Scaled)')
        plt.ylabel('Glucose (Scaled)')
        plt.grid(True)
        plt.show()

    def show_classification_form(self):
        clear_output()
        display(HTML("""
        <div class="result-area">
            <h3>Diabetes Classification</h3>
            <p>Please enter the patient's information to classify diabetes:</p>

            <div class="input-form">
                <div class="input-row">
                    <div class="input-label">Pregnancies:</div>
                    <div class="input-field"><input type="number" id="pregnancies" min="0" max="20" value="0"></div>
                </div>

                <div class="input-row">
                    <div class="input-label">Glucose:</div>
                    <div class="input-field"><input type="number" id="glucose" min="0" max="200" value="100"></div>
                </div>

                <div class="input-row">
                    <div class="input-label">Blood Pressure:</div>
                    <div class="input-field"><input type="number" id="bp" min="0" max="150" value="70"></div>
                </div>

                <div class="input-row">
                    <div class="input-label">Skin Thickness:</div>
                    <div class="input-field"><input type="number" id="skin" min="0" max="100" value="20"></div>
                </div>

                <div class="input-row">
                    <div class="input-label">Insulin:</div>
                    <div class="input-field"><input type="number" id="insulin" min="0" max="900" value="80"></div>
                </div>

                <div class="input-row">
                    <div class="input-label">BMI:</div>
                    <div class="input-field"><input type="number" id="bmi" min="0" max="70" step="0.1" value="25.0"></div>
                </div>

                <div class="input-row">
                    <div class="input-label">Diabetes Pedigree Function:</div>
                    <div class="input-field"><input type="number" id="pedigree" min="0" max="3" step="0.01" value="0.5"></div>
                </div>

                <div class="input-row">
                    <div class="input-label">Age:</div>
                    <div class="input-field"><input type="number" id="age" min="0" max="120" value="30"></div>
                </div>

                <button class="predict-btn" onclick="google.colab.kernel.invokeFunction('notebook.classifyDiabetes', [
                    parseInt(document.getElementById('pregnancies').value),
                    parseInt(document.getElementById('glucose').value),
                    parseInt(document.getElementById('bp').value),
                    parseInt(document.getElementById('skin').value),
                    parseInt(document.getElementById('insulin').value),
                    parseFloat(document.getElementById('bmi').value),
                    parseFloat(document.getElementById('pedigree').value),
                    parseInt(document.getElementById('age').value)
                ], {})">Classify Diabetes</button>
            </div>

            <div id="classificationResult"></div>
        </div>
        """))

    def classify_diabetes(self, pregnancies, glucose, bp, skin, insulin, bmi, pedigree, age):
        # إنشاء مصفوفة من المدخلات
        input_data = [pregnancies, glucose, bp, skin, insulin, bmi, pedigree, age]

        # الحصول على التنبؤ
        prediction = self.analyzer.predict_diabetes(input_data)

        # عرض النتيجة
        result_class = "diabetic" if prediction == "مصاب بالسكري" else "non-diabetic"
        result_html = f"""
        <div class="prediction-result {result_class}">
            <h4>Classification Result:</h4>
            <p>The patient is <strong>{prediction}</strong></p>
        </div>
        """

        display(Javascript(f"""
        document.getElementById('classificationResult').innerHTML = `{result_html}`;
        """))

# === [5] كولباك ربط الأزرار ===
def show_data():
    ui.show_data()

def run_clustering():
    ui.run_clustering()

def perform_search(query):
    ui.perform_search(query)

def show_classification_form():
    ui.show_classification_form()

def classify_diabetes(pregnancies, glucose, bp, skin, insulin, bmi, pedigree, age):
    ui.classify_diabetes(pregnancies, glucose, bp, skin, insulin, bmi, pedigree, age)

output.register_callback('notebook.showData', show_data)
output.register_callback('notebook.runClustering', run_clustering)
output.register_callback('notebook.performSearch', perform_search)
output.register_callback('notebook.showClassificationForm', show_classification_form)
output.register_callback('notebook.classifyDiabetes', classify_diabetes)

# === [6] تشغيل الواجهة ===
ui = DiabetesUI()

# === [1] المكتبات والتجهيز الأساسي ===
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.cluster import KMeans
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score
from IPython.display import display, HTML, clear_output
from google.colab import output
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
import os
import glob
from sklearn.metrics.pairwise import cosine_similarity
from IPython.display import Javascript

# === [2] تحميل البيانات ===
def load_diabetes_data():
    url = "https://raw.githubusercontent.com/plotly/datasets/master/diabetes.csv"
    return pd.read_csv(url)

# === [3] كلاس التحليل ===
class DiabetesAnalyzer:
    def __init__(self):
        self.df = load_diabetes_data()
        self.dt_model = None
        self.knn_model = None
        self.scaler = None
        self.kmeans = None

    def train_compare_models(self):
        X = self.df.drop('Outcome', axis=1)
        y = self.df['Outcome']
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

        # Decision Tree Model
        self.dt_model = DecisionTreeClassifier(
            max_depth=5,
            min_samples_split=10,
            min_samples_leaf=5,
            class_weight='balanced'
        )
        self.dt_model.fit(X_train, y_train)
        y_pred_dt = self.dt_model.predict(X_test)

        # K-Nearest Neighbors Model
        self.knn_model = KNeighborsClassifier(n_neighbors=3)
        self.knn_model.fit(X_train, y_train)
        y_pred_knn = self.knn_model.predict(X_test)

        metrics = {
            'Decision Tree': {
                'accuracy': accuracy_score(y_test, y_pred_dt),
                'precision': precision_score(y_test, y_pred_dt),
                'recall': recall_score(y_test, y_pred_dt)
            },
            'KNN': {
                'accuracy': accuracy_score(y_test, y_pred_knn),
                'precision': precision_score(y_test, y_pred_knn),
                'recall': recall_score(y_test, y_pred_knn)
            }
        }
        return metrics

    def perform_clustering(self):
        X = self.df.drop('Outcome', axis=1)
        y_true = self.df['Outcome']

        self.scaler = StandardScaler()
        X_scaled = self.scaler.fit_transform(X)

        self.kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)
        clusters = self.kmeans.fit_predict(X_scaled)

        accuracy = accuracy_score(y_true, clusters)
        accuracy_flipped = accuracy_score(y_true, 1 - clusters)
        final_accuracy = max(accuracy, accuracy_flipped)

        precision = precision_score(y_true, clusters)
        recall = recall_score(y_true, clusters)

        results = {
            'accuracy': final_accuracy,
            'precision': precision,
            'recall': recall,
            'clusters': clusters
        }
        return results

    def predict_diabetes(self, input_data):
        input_array = np.array([input_data])
        proba = self.dt_model.predict_proba(input_array)[0][1]

        glucose = input_data[1]
        bmi = input_data[5]
        age = input_data[7]

        if (glucose > 140 or
            (glucose > 126 and bmi > 30) or
            (glucose > 130 and age > 45) or
            proba > 0.45):
            return "مصاب بالسكري"
        else:
            return "غير مصاب بالسكري"

# === [4] واجهة المستخدم ===
class DiabetesUI:
    def __init__(self):
        self.analyzer = DiabetesAnalyzer()
        self.analyzer.train_compare_models()
        self.create_interface()
        self.vectorizer = None
        self.df_tfidf = None
        self.file_names = None

    def create_interface(self):
        html = """
        <style>
            .google-container { font-family: Arial, sans-serif; text-align: center; margin-top: 50px; max-width: 800px; margin: auto; }
            .google-logo { font-size: 60px; font-weight: bold; margin-bottom: 20px; }
            .G { color: #4285F4; }.o1 { color: #EA4335; }.o2 { color: #FBBC05; }.g { color: #34A853; }.l { color: #EA4335; }.e { color: #4285F4; }
            .search-container { margin: 20px auto; max-width: 600px; }
            .search-box { width: 100%; padding: 12px 20px; margin: 8px 0; border: 1px solid #dfe1e5; border-radius: 24px; font-size: 16px; box-shadow: 0 1px 6px rgba(32,33,36,0.28); }
            .google-btn { background-color: #f8f9fa; border: 1px solid #f8f9fa; border-radius: 4px; color: #3c4043; padding: 10px 20px; margin: 0 5px; cursor: pointer; transition: all 0.3s; }
            .result-area { margin-top: 40px; padding: 20px; text-align: left; border-top: 1px solid #ebebeb; max-width: 800px; margin: auto; }
            .metrics-table { width: 100%; border-collapse: collapse; margin: 20px 0; }
            .metrics-table th, .metrics-table td { border: 1px solid #ddd; padding: 8px; text-align: center; }
            .metrics-table th { background-color: #f2f2f2; }
            .input-form { margin: 20px 0; padding: 20px; border: 1px solid #eee; border-radius: 8px; background-color: #f9f9f9; }
            .prediction-result { margin-top: 20px; padding: 15px; border-radius: 4px; font-weight: bold; text-align: center; }
            .diabetic { background-color: #FFCDD2; color: #C62828; }
            .non-diabetic { background-color: #C8E6C9; color: #388E3C; }
        </style>
        <div class="google-container">
            <div class="google-logo">
                <span class="G">G</span><span class="o1">o</span><span class="o2">o</span>
                <span class="g">g</span><span class="l">l</span><span class="e">e</span>
            </div>
            <div class="search-container">
                <input type="text" id="searchQuery" class="search-box" placeholder="Search documents...">
            </div>
            <div class="buttons">
                <button class="google-btn" onclick="google.colab.kernel.invokeFunction('notebook.showData', [], {})">Information Retrieval</button>
                <button class="google-btn" onclick="google.colab.kernel.invokeFunction('notebook.runClustering', [], {})">Clustering</button>
                <button class="google-btn" onclick="google.colab.kernel.invokeFunction('notebook.showClassificationForm', [], {})">Classification</button>
            </div>
            <div id="resultArea" class="result-area"></div>
            <div id="plotArea" style="margin-top: 20px;"></div>
        </div>
        """
        display(HTML(html))

    def show_data(self):
        clear_output()
        display(HTML("<div class='result-area'><h3>Information Retrieval System</h3></div>"))

        folder_path = "/content/"
        file_paths = glob.glob(os.path.join(folder_path, "*.txt"))

        if not file_paths:
            display(HTML("<div class='result-area'><p>No text files found</p></div>"))
            return

        documents = []
        self.file_names = []
        for file_path in file_paths:
            with open(file_path, "r", encoding="utf-8") as file:
                documents.append(file.read())
                self.file_names.append(os.path.basename(file_path))

        self.vectorizer = TfidfVectorizer()
        tfidf_matrix = self.vectorizer.fit_transform(documents)
        self.df_tfidf = pd.DataFrame(tfidf_matrix.toarray(),
                                    columns=self.vectorizer.get_feature_names_out(),
                                    index=self.file_names)

        display(HTML("<div class='result-area'><h4>TF-IDF Matrix:</h4></div>"))
        display(self.df_tfidf.head())
        self.df_tfidf.to_csv("tfidf_result.csv", index=True)
        display(HTML("<div class='result-area'><p>✅ Results saved to 'tfidf_result.csv'</p></div>"))

        search_html = """
        <div class="search-container">
            <input type="text" id="searchQuery" class="search-box" placeholder="Enter search query...">
            <button class="google-btn" onclick="google.colab.kernel.invokeFunction('notebook.performSearch', [document.getElementById('searchQuery').value], {})" style="margin-top: 10px;">Search</button>
        </div>
        """
        display(HTML(search_html))

    def perform_search(self, query):
        clear_output()
        self.show_data()

        if query:
            query_vector = self.vectorizer.transform([query]).toarray()
            tfidf_matrix = self.df_tfidf.to_numpy()
            cosine_similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()
            sorted_indices = np.argsort(cosine_similarities)[::-1]
            sorted_indices = [i for i in sorted_indices if cosine_similarities[i] > 0]
            sorted_files = [self.df_tfidf.index[i] for i in sorted_indices]

            if sorted_files:
                result_html = "<div class='result-area'><h4>Search Results:</h4><ul>"
                for i, doc in enumerate(sorted_files):
                    result_html += f"<li>{doc} (Similarity: {cosine_similarities[sorted_indices[i]]:.4f})</li>"
                result_html += "</ul></div>"
                display(HTML(result_html))

                metrics_html = """
                <div class='result-area'>
                    <h4>Performance Metrics:</h4>
                    <p>Precision: 28.57% (0.2857)</p>
                    <p>Recall: 66.67% (0.6667)</p>
                    <p><small>Note: Demonstration values</small></p>
                </div>
                """
                display(HTML(metrics_html))
            else:
                display(HTML("<div class='result-area'><p>No matches found</p></div>"))
        else:
            display(HTML("<div class='result-area'><p>Please enter query</p></div>"))

    def run_clustering(self):
        clear_output()
        results = self.analyzer.perform_clustering()

        html = f"""
        <div class="result-area">
            <h3>Clustering Results</h3>
            <table class="metrics-table">
                <tr><th>Metric</th><th>Value</th></tr>
                <tr><td>Accuracy</td><td>{results['accuracy']*100:.2f}%</td></tr>
                <tr><td>Precision</td><td>{results['precision']*100:.2f}%</td></tr>
                <tr><td>Recall</td><td>{results['recall']*100:.2f}%</td></tr>
            </table>
        </div>
        """
        display(HTML(html))

        X_scaled = self.analyzer.scaler.transform(self.analyzer.df.drop('Outcome', axis=1))
        plt.figure(figsize=(8,6))
        plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=results['clusters'], cmap='viridis', alpha=0.6)
        plt.title('K-Means Clustering')
        plt.xlabel('Pregnancies (Scaled)')
        plt.ylabel('Glucose (Scaled)')
        plt.grid(True)
        plt.show()

    def show_classification_form(self):
        clear_output()
        display(HTML("""
        <div class="result-area">
            <h3>Diabetes Classification</h3>
            <p>Please enter the patient's information to classify diabetes:</p>
            <div class="input-form">
                <div class="input-row">
                    <div class="input-label">Pregnancies:</div>
                    <div class="input-field"><input type="number" id="pregnancies" min="0" max="20" value="0"></div>
                </div>
                <div class="input-row">
                    <div class="input-label">Glucose:</div>
                    <div class="input-field"><input type="number" id="glucose" min="0" max="200" value="100"></div>
                </div>
                <div class="input-row">
                    <div class="input-label">Blood Pressure:</div>
                    <div class="input-field"><input type="number" id="bp" min="0" max="150" value="70"></div>
                </div>
                <div class="input-row">
                    <div class="input-label">Skin Thickness:</div>
                    <div class="input-field"><input type="number" id="skin" min="0" max="100" value="20"></div>
                </div>
                <div class="input-row">
                    <div class="input-label">Insulin:</div>
                    <div class="input-field"><input type="number" id="insulin" min="0" max="900" value="80"></div>
                </div>
                <div class="input-row">
                    <div class="input-label">BMI:</div>
                    <div class="input-field"><input type="number" id="bmi" min="0" max="70" step="0.1" value="25.0"></div>
                </div>
                <div class="input-row">
                    <div class="input-label">Diabetes Pedigree Function:</div>
                    <div class="input-field"><input type="number" id="pedigree" min="0" max="3" step="0.01" value="0.5"></div>
                </div>
                <div class="input-row">
                    <div class="input-label">Age:</div>
                    <div class="input-field"><input type="number" id="age" min="0" max="120" value="30"></div>
                </div>
                <button class="predict-btn" onclick="google.colab.kernel.invokeFunction('notebook.classifyDiabetes', [
                    parseInt(document.getElementById('pregnancies').value),
                    parseInt(document.getElementById('glucose').value),
                    parseInt(document.getElementById('bp').value),
                    parseInt(document.getElementById('skin').value),
                    parseInt(document.getElementById('insulin').value),
                    parseFloat(document.getElementById('bmi').value),
                    parseFloat(document.getElementById('pedigree').value),
                    parseInt(document.getElementById('age').value)
                ], {})">Classify Diabetes</button>
            </div>
            <div id="classificationResult"></div>
        </div>
        """))

    def classify_diabetes(self, pregnancies, glucose, bp, skin, insulin, bmi, pedigree, age):
        input_data = [pregnancies, glucose, bp, skin, insulin, bmi, pedigree, age]
        prediction = self.analyzer.predict_diabetes(input_data)
        result_class = "diabetic" if prediction == "مصاب بالسكري" else "non-diabetic"
        result_html = f"""
        <div class="prediction-result {result_class}">
            <p>The patient is <strong>{prediction}</strong></p>
        </div>
        """
        display(Javascript(f"""
        document.getElementById('classificationResult').innerHTML = `{result_html}`;
        """))

# === [5] Callbacks ===
def show_data(): ui.show_data()
def run_clustering(): ui.run_clustering()
def perform_search(query): ui.perform_search(query)
def show_classification_form(): ui.show_classification_form()
def classify_diabetes(pregnancies, glucose, bp, skin, insulin, bmi, pedigree, age):
    ui.classify_diabetes(pregnancies, glucose, bp, skin, insulin, bmi, pedigree, age)

output.register_callback('notebook.showData', show_data)
output.register_callback('notebook.runClustering', run_clustering)
output.register_callback('notebook.performSearch', perform_search)
output.register_callback('notebook.showClassificationForm', show_classification_form)
output.register_callback('notebook.classifyDiabetes', classify_diabetes)

# === [6] Run UI ===
ui = DiabetesUI()



#1. المعايير الطبية الرئيسية
#يتم تشخيص المريض كـ "مصاب بالسكري" إذا تحقق أيٌّ من هذه الشروط:

#الجلوكوز > 140 (مستوى السكر في الدم بعد الصيام ≥ 140 ملغ/دل يعتبر تشخيصًا للسكري).

#الجلوكوز > 126 مع BMI > 30 (سكر مرتفع + سمنة).

#الجلوكوز > 130 مع عمر > 45 سنة (عمر متقدم مع سكر مرتفع).

#2. تنبؤ نموذج الذكاء الاصطناعي
#يستخدم النموذج (شجرة القرار + KNN) احتمالية الإصابة:
#إذا كانت احتمالية الإصابة > 45% (حتى لو كانت القيم أقل من العتبات الطبية)